probably is the are the years of AI accountability and that is what has been the theme across most of the sessions which uh I've seen in this uh summit and we are integrating our think these thinking machines uh into the fabric of our daily lives today whether it is from the infrastructure to our personal identity one fundamental question arises how do We innovate at the speed of the thought without compromising the safety of our soul. The safe and trusted AI dialogue is not just a technical discussion. It is a social contract of the 21st century. We are not just building software. We are building the digital foundation of trust for 1.4 billion Indians and by extension the global community. To guide our discussion today, I want to frame the discussion around three critical tensions or dilemmas. Innovation versus regulation. Can government governance uh be a trust accelerator rather than a bureaucratic break? Sovereignity versus interoperability. How do we honor cultural uh sutras while ensuring global digital train remains secular? claimed versus verified. How do we know from move from trust me uh it is safe to here is the proof that it is secure. Today as you can see we are joined by a power panel that represents the entire life cycle of this challenge from the architect of EU AI act and the India AI mission uh to the leader who scale these technologies across the world's largest enterprises and creative frontiers. It is it is my priv distinct honor to introduce our panelist uh Mr. DG uh DG Roberto Violas has not been able to join us so far [clears throat] but it is my privilege to introduce a very distinguished senior colleague from my previous life Mr. Alkesh Kumar Sharma is retired. Sir is the former secretary of meeting. Uh I think all of he needs no further introduction. Uh I think all of us know about what all he has done but I leave it uh to a later minute for this. Mr. Gurani is supposed to join very soon. He's on his way stuck in traffic jam and I can take part of that responsibility being former uh uh on behalf of my colleagues in Delhi police. I was part of Delhi police for 32 years. So you can blame it on me also Mr. Ajit Kumar CIO of HCL technology [applause] Dr. Monisha regional consulting leader Southeast Asia for Mandian consulting based out of Singapore and Mr. Abina Verma Khalidini is that right way to say [applause] CEO of Icon Studios and I have heard what wonderful work he's doing and before we move forward I'll request him to tell us what he's doing. Out of sight, out of mind. [laughter] >> Some sometimes our subconscious is at work. I work for Google. She works for Microsoft. [applause] So please please excuse me if people on the competitive spirit. uh Miss Charu Sinasan is GVP at Microsoft and but before going forward I'll request uh all the panelists to kindly introduce yourself more in terms of how does how do they look at this topic today from the prism of their experience and I'll start with Mr. Abhinav I think uh to me it appears one of the most interesting things which you are doing today. So why don't you start Abhinav? >> Hello thank you. Thank you so much uh for that introduction kind introduction and it's a complete honor to be present among such luminaries today and have a chance to speak on this topic. Uh so before I go into the topic itself I want to give a quick uh intro what what I do because it's not uh very traditional but uh we basically uh work at the intersection of culture and technology. So we license uh the IP rights AI IP rights of some of the cultures biggest icons in the country where we essentially and one of the biggest icon that we've worked with is Mr. Amitab Bachan. So we've licensed his exclusive AI rights where the only person or I mean uh entity in the world that can use his likeness uh for creating his digital avatar. It's just it's not about just creating his digital avatar but we own his rights exclus ex exclusively and perpetually. So we are the custodians of his legacy for the next 100 years. We will be able to we are the only uh entity that can create his likeness, his voice, his visual etc. So with that comes a lot of heft and weight right like with such a big legacy and a name to create uh a structure and an AI platform that basically has provenence and it has uh clear controls on what we generate how we generate etc >> and that indicates the trust of Mr. Amitab Bachan in you and your company and most importantly in the technology. >> Yes. And he's been very kind and gracious with his uh time and he's given us a lot of time because this was not the uh it was not 2025. This was 2022 right when Chad GPT was just coming out and text generation was only available but we were the pioneers to create a digital avatar that looked like him, feel like him, talk like him etc. So was quite he's very generous and gracious with his time with us. >> Thank you. Thank you. I'll now request Mr. Rajit Kumar to introduce himself and uh talk about briefly about the topic from your experience. >> Uh good evening everyone and first of all thank you very much for including me as a part of this August panel uh to talk about uh an extremely important topic. My background is I've been in technology for the last uh 25 years 25 plus years uh for for Accenture initially and then with HCL technologies uh both on the market facing side which means select selling our services to customers as well as now more importantly in terms of how do you leverage this technology to tech various technologies including AI in order to drive uh differentiated performance within the company. So HCL as a corporate entity um I think uh it's quite paradoxical that um you know I'm speaking to you at a time when the very existence of companies like HCL are being questioned in the context of what AI can do and could do in the future. So um for us uh AI is in some senses an ex existential threat. uh so we have obviously have got to find out ways and means of transforming ourselves to help ourselves and help our customers leverage this technology but coming more importantly to this topic I think uh Mr. Madan rightfully mentioned that gone are the days of PC's and proof of and concepts. Now it's all about basically rolling out use cases and capabilities at scale. And what this technology does is basically when leveraged to its full capability is to be able to take probabilistic outcomes and being able to generate uh subsequent actions. And the word probabilistic outcomes means that there is got to be a tremendous level of trust and responsibility because if you don't have enough information to make a subsequent decision then as a trusted adviser it is important that the tool is able to recognize its limitation and be able to articulate that outcome appropriately. So as we look at rolling out this technology, the whole aspect of how do you ensure that there is trust and there is responsible use of the outcomes of this probabilistic model would become more and more critical. Now these as you can see both at the uh at Barat Mandup as well as probably in other locations there is enormous amount of effort, time and money going into building solutions and it is very important for all stakeholders to define standards so that basically the whole of what we expect in terms of responsible use, what we expect in terms of security, what we expect in terms of privacy are clearly defined, measurable and can be then verified. So I think this is a very apt topic in the context of AI scaling out and I think there is an immense responsibility to various stakeholders to enable this journey in a in a in a in a smooth and frictionless manner. >> Thank you Mr. Kumar. Let me make amends for my earlier mistake. I'll request charu Mr. Nasan if you could introduce the topic from your experience. Assuming uh this is not the question you're going to ask me right >> uh it is just a introduction we are just be brief okay I'll try and be brief so hi everyone charasel let me introduce myself given Dr. Madan failed to the previous I'm just kidding but uh I build I build uh Azure um this is my day job I'm an engineer by profession um so my role at Microsoft is to build the cloud infrastructure that enables AI workloads to run on top with safety privacy and security right that's really how I think of my day job um and this topic is of significant interest to us not just as as Microsoft but as technology players that given um I think Every large foundational change in humanity has reshaped lives. Whether it was steam engine, whether it's electricity which reshaped kind of our day-to-day lives. I think AI is going to impact every possible sector we can think of. And every such change needs to roll out with I think the human beings who going to be impacted by that change completely involved and we're hoping uh this conversation kind of allows us to move the dial forward in terms of thinking. So I'm such an honor to be part of such a distinguished panel. Uh most specifically the work I do uh on a day-to-day basis uh we uh build something called confidential compute uh where we really think that at the core of uh all this privacy risk which I think everyone's care cares deeply about. You need to build technology that no human can subvert, right? So you want to kind of uh build a zero trust kind of security model where you don't even trust the cloud operator whether that's Microsoft or Google, right? and make sure that at the end of the day uh you can run uh run your AI workloads with with this guarantee and almost like a legal a technolal guarantee that uh there is no privacy risk there is no data exfiltration that AI models are not training on your data that you can inference in a way that preserves confidentiality I'll give you a couple of examples hopefully when I get asked my question um but then I just thought I'll share that this topic is of deep interest to us and I think we're working not just It's not just a technology problem. It's I think it's the intersection of the policy intent, the technology kind of backbone and of course the ecosystem. And we think when these three come together then magic unfolds. And I think this technology more than anything else before needs these three things to kind of come together. With that I'm going to hand off to >> Thank you. Thank you Mr. Nasan. I'll request the second lady in the group Dr. Monisha if you could introduce the topic from your experience. >> Absolutely. Thank you very much Dr. Broy. Um so uh coming I come from Mandant. I'm the Southeast Asia consulting leader and uh as far as Mandant is concerned you know our our basically our mission is to make sure that every organization is secure from cyber threats and we ensure that they're ready for it. Uh we were recently acquired about two three years back by Google cloud. So now we are part of the Google cloud uh you know family and with that you know our our aim is now to make sure that the frontline intelligence and the enterprise scale technology are actually working together. Um mandent is basically I'm I'm sure most of you here are aware but I'll just reiterate uh our identity is basically built on the work that we do as first responders for the most sophisticated breaches across the world. So uh our mission is not just to provide you know the um uh consulting but it is also to make sure that what we learn from the trenches across the world as we are solving and helping clients in their breaches we bring it to every client that we work on. So I think today's topic is very interesting and I'm so honored it's a pleasure and a privilege to be a part of such an august uh you know uh team here panel here. So thank you very much for having me. >> Thank you. Thank you Dr. Malaysia and I have deliberately kept the best for the last. Uh I'll request uh my senior in the service uh and a man who has been responsible for lot of things which we are discussing not only in this panel but across so many panels in this summit uh as one of the main architects of so many things which are happening. Sir, would you like to uh uh give your thoughts on the briefly your thoughts on the topic and the importance of >> Thank you. Uh I think let me at the outset congratulate GCTC and of course the imminent panelists uh they have made me a part of this because see we are the government so when you find the people sitting on all my sides they try to unleash AI and we try to see that how we should control it you know [laughter] because And that is what India has done when we started uh working on the digital public infrastructure the trust safe safety trust accountability and resilience these have been the foundation foundational principles on which we started DPI and I think that continues because you can't lose out on that and we found that when we talk about safety and trust or secure entrust. These are not abstract ideals. We made it functional necessities in whatever we did. So when we started EPI we said yes while we want it to be open source interoperable expendable customizable but we always ensure that these four components should be part of the foundational principles and the same thing we followed in AI policy AI systems. So, so when we look at trust and safety, I think uh when AI has become and and I will be candid that when we started 21 when I joined the ministry, uh we had the semiconductor mission AI was still in the beginning we had a small AI India AI small commitment and then we started a national program and when we were talking about national program in 2022 and that is when uh the at the highest level we told that this is too small a cabinet note the amount which we proposed and the prime minister said it's too small an amount why don't you go into billions and that is how the AI mission was conceived and had a fairly big amount so when we look at these things they have always been part of and you can't leave and in today's world when when AI has become all pervasive I mean there is no sector even in India because we have already come out with India tech on PPI and and all these things need AI. You look at India's uh and the impact which this AI can create today. It it can be humongous. You know, you look at the impact in terms of adverse impact. I mean one small bad innovation can can really create havoc. You look at the scale at which AI is being done. We look at the sovereign AI. I mean that's something which countries are talking about. while we probably have to do something about that but we still have to make it interoperable as uh and and and the resilience should be an integral part. So these were some of the things which we had. So when this uh uh DCTC suggested that they have one of the sessions on this so I thought uh I must say yes because it is not only that I will be able to you share my experience of how we we moved from the basic digital infrastructure to DPIs and then on to AI mission. But it will be great to learn from from my colleagues in this panel that what is more that needs to be done and how government should become little more proactive and at the same time see how do we calibrate the the AI the ball out to ensure that it is still safe trusted accountable and resilient. Thank you. >> Thank you. Thank you sir. Mr. Kumar for a global CIO the shared future depends on responsible deployment. What is one foundational shift you are seeing in how large enterprises manage accountability especially in security sens sensitive domains like public infrastructure. Yeah. So um I would answer this question from the perspective of both our customers as well as internally within HCL. uh today as I mentioned earlier the the the corporations are moving from proof of concepts into large scale deployment of AI based solutions to solve their most pressing and critical problems. I think to answer your question u as to what is the significant shift we are seeing. I think the significant shift we are seeing is everybody recognizes that while speed is of essence to be able to identify and be able to conceptualize solution and deploy solutions for our most pressing business problems. There is a clear understanding that this technology if not used properly can cause a lot of unintended consequences. So what are corporations doing in order to ensure that you don't fall into the trap? I think there are a couple of things. The first is they are beginning to define standards as to what do they expect from their models from various dimensions of security, privacy and responsible use. The second thing we're looking we are seeing companies beginning to employ or deploy is processes which will manage the life cycle of building solutions where these principles of responsible use, privacy, security etc are enforced. So it doesn't become an afterthought that you have built the model then we will start thinking about these things. The idea is that the moment you start thinking about an idea, these aspects become an integral part of the journey. So it's no longer an afterthought. The third is I think creating specific structures within the organization whose responsibility is to ensure that these tenants are applied in a very deliberate manner to the extent that solutions cannot be deployed into production environments unless it passes the test from a responsible secure and um you know confidentiality perspectives. So that's the third dimension of the change we are seeing. And finally is the emergence of technology to enforce these tenants. So this is no longer somebody just defining standards and then administering a checklist at various stages in the life cycle of building a generative AI based use case. It is about embedding these tenants into the underlying technology. So when you build a solution, these things become automatically a part of that solution. This is much like in the good old way of software development embedding technology so that security so that coding practices etc etc are deployed at the time of building the solution. So the answer is that responsible use of these technologies is no longer an afterthought. It is a very deliberate um process in most companies where there is a strategy, there is processes, there is technology and there is an organization to enforce these tenants. >> Thank you. Thank you. You wanted to say more? >> No. >> Uh embedding the idea of responsible, safe and secure AI across the life cycle right from the ideation stage. Uh thank you. Thank you Mr. Kumar for bringing out this idea. Abhinav uh we are at the you are at the frontier of AI creating digital personas. How do we ensure that democratic values and cultural sensitivities? I'm deliberately asking the uh this in context of culture sensitivities because there's lot of uh lot of noise around uh uh this debate that we the it has to be tailored to the local sensitivities local cultural nuances and even language uh dialects etc. So how do you ensure that democratic values and culture sensitivities are encoded into these systems to prevent the erosion of truth and personal identity? >> Thank you for the question. Uh just to add a little bit about uh digital personas. Uh we feel that digital personas of your own self are an extension of human rights itself. uh but today's world of technology I think it's very easy to create something u so I believe that uh you know systems and guardrails have to be placed uh at the point of creation as uh as he was mentioning where you know every single digital file if it can have a provenence of where it was created if it is authentic uh it doesn't exist today we've not been able to do it in social media so what I'm suggesting or what I'm alluding to is uh is not something that can be built like uh in day in one day but I think having provenence of every single digital file or a digital persona that's being created and when it was created how it was created and how the interactions happened uh I think that will solve a lot of the issues surrounding uh you know right now there's a big debate on like uh deep fakes right um I always joke and say that we are like a deep original company right we actually go out and you know license these rights to create uh you know content or AI based experiences based on you know uh controls that we place uh with the celebrities or with the iconic personalities that we have. Every single piece of content that is generated uh for the experience that we're creating it uh is actually vetted by the personality and actually approved by the personality. So if there's a digital system that can be built for all types of content where provenence uh can be established, I think that will actually solve a lot of the issues surrounding digital personas today because it's become very easy. 2022 it was not that not that easy but today anyone and everyone can create it and it's actually very tough to find who created this piece of content or technology. uh and coming to the part of how we are encoding these cultural values into the systems itself and I believe that a lot of sovereign AI systems are being built right now. I'm like very excited about how uh and when we can use these technologies into our stack. So we're really looking for like multilingual systems that can actually understand because uh one of our earliest deployments have been in banks. So our systems we just don't create content and place them on social media. Our content like Mr. Bachchan you can walk into I I don't want to name the bank but you can walk into the bank and you're able to talk to Mr. Bachan and automate services right so you're walking into an institution that is highly trusted uh so any type of people can walk into a bank right like it could be people who don't speak English who could speak like very regional languages so we had to have systems that are in place that understand regional context not only uh not only language but also regional context of where they're from. So we specifically designed a system. We didn't keep it very open-ended. As of today, we've kept the system very uh I would say in this case very limited because we are actually waiting for sovereign models to develop enough so that we can actually open it up. I would say that we we've developed 15 services automations where like you can get a checkbook, you can get a uh you can can get a bank statement, you can get a lot of services done by talking to these personalities or AIs that we created inside these banking branches. So I think uh we're waiting for sovereign models to actually come and uh see how we can embed them into the into our systems to be able to understand context as of late. >> Thank you Abhinav. And uh your mention about the digital personas uh brings me to uh 2023 when Interpol was launching its uh metavas platform at Daros and I was moderating a panel and one of the important point came how do we treat digital personas in terms of are they an extension of for example in your case Mr. Rammitab Bachan is it an extension of that? Is it a separate legal entity? Who is uh who do we uh in case of going back to my role of enforcer? Who do we uh take into uh whom do we charge in case of any problem or any wrongdoing by this persona? Is it Mr. Amitab Bachan? Is it you or uh who who would be responsible? >> I think uh I mean great question. um we intentionally kept every single piece of content three-way approved so that everyone in the loop is involved in approving the approving what the experience went that includes Mr. Bachan's team that includes the icons team and that includes the bank as well. So there are three parties or any any three parties that are getting this done. So all three parties were were involved in the entire approval process. So we believe that having this very transparent traceable way of approvals uh make sure that you know the experience was approved by all parties involved and no unilateral party has taken a decision to actually put this experience or put this technology out. So we've created that provenence at least on a very limited scale. To scale that out we will definitely have to think about another platform which can be cryptographically uh enforced like a immutable transaction between the parties where you know every single party uh which is a party to that uh experience has approved this and without that I think uh I think that's the way to go. >> Thank you. Thank you. >> Uh sorry for interruption sir. will have some couple of seconds to uh group photo. We are waiting for another particip panelist but they are unable to come now. So we are about to reach our session and so just uh 30 seconds we can stand up and have some group photo with the uh panelist then we can continue sir. Thank you so much, sir. We can continue. Thank you for giving me the opportunity to think about the next question. Uh so uh Mr. Reasan uh trust is built through auditability and Abhinav was mentioning about uh trans traceability also. So picking up from there uh trust is built through auditability and clear controls but from a generic engineering point of view >> how do we build transparency into the AI life cycle so that it is verified rather than it just claimed >> great question and uh I love the examples uh I mean I've used especially around how the uh it's a self-expression of the artist right and how you that's so core to preserve that and um interestingly in today's world like whether you look at finance, you look at healthcare, you look at any industry for that matter, it involves sensitive data, right? When you look at uh some of the examples uh that come to mind for for a bunch of our hospitals are trying to kind of come together bringing their kind of oncological uh like this is data for uh cancer research together and like improve their disease models. But how do these hospitals come together in a way that they don't dulge patient privacy issues, right? And uh the interesting thing is obviously every hospital wants to show up to be part of this consortium. The the model needs to get better, but how do they collaborate without exposing patient information? And to me, this is really why I think the confidential computing example that in fact Mr. Ajit also talked about is super important. And in fact, we're building something called confidential clean rooms. This is a very interesting tech where multiple parties can essentially come into the clean room and you have a guarantee you have cryptographically uh sound approaches that say that the data there was no privacy there's no excfiltration of data outside the clean rule right and how do you kind of create attestation reports how do you create in a sense a techno legal construct something that can stand up in a court of law so this is really where I think technology meets uh regulators another very interesting interesting example like uh this is in the financial sector where we see uh lenders and banks kind of come together because they want to kind of assess the credit risk scores of consumers. How do you do this? Again I think I want to go back to u the depa which is like a very important uh I'd say DPI that it's becoming a DPI now for AI. Uh DEPA 1.0 kind of proved to us that you could use uh user consent kind of layer that into like these national scale platforms and really uh Mr. Charles I think uh the work in the DPI space is just so foundational for the country and but as we look at depart 2.0 they're kind of piloting how do you kind of do this for training and inferencing how do you do training on confidential infrastructure right how do you do inferencing so if you want to kind of inference from a either a foundational model how do you ensure again that privacy is preserved so to me I think um in fact we're working uh pretty closely with Iceber on uh the DEPA work uh we think it's really important to have these operate like as digital rails for the rest of the for the entire economy And uh to me unless policy makers kind of come together with regulatory bodies and we form these consortiums I think it's going to be important to make sure that the consortium members also collaborate with confidentiality preserved right you don't want you want immutable ledgers you want all those technical building blocks that make the use case Mr. have talked about kind of come come alive and so to me I think it's the intersection of policy the technological kind of technical constructs and of course the ecosystem uh where this magic unfolds so I hope that answered your question >> thank you thank you Mr. And just a follow-up question you mentioned about consortiums and also bringing together people would you also like to comment on collective responsibility not only in those amongst those who are joining the consortium but at a wider scale uh collective responsibility between users developers uh >> absolutely I think uh like we said every technology needs to be shaped by the humans around like I I really loved uh you Mr. when you talking about the need to have resilience to have privacy kind of baked in as opposed to having these runaway AI models right so to me I think the there is huge responsibility in the model developer the model user right you need to have user data that's handled sensitively ultimately there are risks where the models get better because they've actually consumed data the things that bother me are really around representation what about the use case you talked about multicultural What about if a disease model doesn't represent the panindia context right now? Do you have some subset of patients that are underserved because the disease model was not representative enough? So to think about inclusion, think about fairness, privacy, security, accountability, and Microsoft has a bunch of what we call the responsible AI principles, but these aren't just theoretical principles. They need to become engineering constructs. So we're working hard at that and continue to kind of stay tuned as we look at. >> Thank you. Thank you, Mr. Nasan we have been talking about privacy we have been talking about exfiltration we have been talking about security so let me go to Mandian Dr. Misha we often talk about AI ethics in terms of bias or transparency from your perspective at Mandian what is the unseen security threat that most people are missing when they discuss safe AI in 2026 >> very good question thank you Dr. Roy uh see while bias and transparency are absolutely critical right and we are seeing that uh while that be the case there's a shift in the adversary because today the adversaries are also using AI and most of our adversaries are highly motivated if not more than the defenders right be the nation state actors be it for politics be it for money for whatever right so they're extremely extremely motivated now the unseen threat that we see is the democratization of sophisticated attacks. Now there was a time when you had to have some kind of knowledge to be able to create campaigns or sophisticated attacks. But today using AI our adversaries without the knowledge of technology without the knowledge of security are able to you leverage AI and create some very sophisticated attacks at a speed at which the defenders may find it very difficult to take care of. So that is an area which has become extremely extremely critical. you know we've moved very from you know there was a time when we were talking about um you know prompt injections etc but we moved way past that today adversaries are looking at agentic shadow AI which means we're actually looking at unmanaged uncontrolled agents which are being deployed and a lot of organization internally are deploying thinking that they're doing it to help themselves not realizing that they're opening up back doors massive back doors for uh you know for large um breaches that to take place. Now at Amand what are we seeing right? So what we have observed that a lot of our uh threat actors and lot of attackers are actually using AI to automate the discovery of zero day vulnerabilities and at such an amazing speed that folks who are looking at the socks of the world the nos of the world they're unable to actually match that speed. So that is becoming very very challenging. Now when we talk about safe and trusted you know a system can be ethical but if it is not safe we're actually leaving a lot open for a number of adversaries to come in especially the nation state actors. So I think uh this goes hand in hand and it is extremely important that we look at this unseen or the democratization of uh you know the attackers and the sophisticated attacks here. >> Thank you. Thank you Dr. Manisha let me go to sir uh going back to India's unique sovereign capabilities sir the India AI mission is anchored in seven sutras could you please elaborate the governance pillars that make India the use case capital of the world and how we ensure that these systems are secured by design for 1.4 four billion people. >> Yeah, thank you. I think even if you look at our uh initial uh uh digital systems or digital public infrastructure and even the DPDP act if you look at digital uh you know personal data protection act you know we we always had that uh information privacy data privacy it's the the core principle and we there again we suggested that even in AI system as somebody talked about standards and uh prescriptions we did not want to be prescriptive We always said we'll be principalbased solutions, principlebased regulation. We did not want to go the the G GDPR way, you know. So, so the whole objective was that it should be uh it should be principal based not prescription based. It should not throttle technology. We should continue with innovation. It should we should not stifle innovation as it has been done in some of the geographies. So the whole objective is we the these guiding principles seven sutras which you talked about of governance they are basically begin with the foundation of trust that we must have trust first it should be people should be the front it should help there should be public good so it should be public facing public should be should be able to understand it public should benefit from it we also had focused that it should be innovation It should promote innovation. It should not restrain innovation. We also looked at the fair and inclusive system. It should reach the last mile as Mr. was just now talking about. It should not no should be no one should be left behind which we have done in our UPI. we have done in other DPIs and it should be design built the security features which we are talking about whether it is trust or safety or resilience it should be built in the design of any AI system which we are creating I think that's one of the principle and of course safety resilience and sustainability so these are seven governance principles on which we had seven AI mission pillars so these will be the guiding principle And then we move on to the next level where we want to create compute capacity. We are creating innovation so that we promote startup culture. We have we have set up India data set the AI course because India has huge quantum of data with 1.42 billion people 50% of the world's transactions we are doing in India. 80% of India's transactions are on UPI on digital transactions. I mean so the kind of data we have whether it is in agriculture, health, education. So we have to create we have to ensure that data is quality data. So the India data set is actually working on that. We are looking at uh to to ensure that there public private partnership which should take it to the next level. It skill upgradation is very important because this is one of the issues time and again people say there will be job losses. We are saying that it needs continuous you know reskilling and upskilling. that's something we need to continue with. So you so the the all these seven pillars of AI mission are also talking about those only and again safety trust and resilience is important part. We have set up an institute there is an institutional mechanism we have AI safety institute which will actually look at right from concept to commissioning how AI systems should ensure these four principles on which we are building up. We have set up an AI governance mechanis committee which will ensure that both center and the state they coordinate well and ensure that data is uh the systems are again interoperable at the same time. They should be they should have resilience and safety and security features built into the whole system. So this this this is this is an inbuilt system. You talked about multilingual somebody was talking about AI bhashni we we started when I started it we did not even have 10 11 languages today we we covered all constitutional languages and I happy to tell you the 36 dialects and languages now AI bhashni is actually doing and most of the banks and most of the institutions are now using in fact in DPDP act and DPDP when you see consent of a person it has to be in a language which he understands So explan explanability understandability is core principle whether it is AI systems or DPDP or DPI these are the core principles on which we work. Thank you. >> Thank you sir. Uh I think two of the most important things which are being discussed today. Uh one is uh the institute which you mentioned rightly mentioned and second is the uh innovation over restraint. I think this is one of the uh these are the two important issues which are being discussed in most of the panels I uh attended and that takes me to the Mr. Gunani and I've already apologized in your absence that uh because of my previous avatar as in Delhi police you were delayed. [laughter] So uh and we talked about the policy and mission and as I was uh telling sir that we should now look at the engine of innovation. Mr. Gonani you have often said that technology must serve people and the planet. How do we ensure that the governance frameworks which have been discussed today and uh in this panel and also elsewhere act as a trust accelerator for industry rather than a bureaucratic break on the speed of innovation. Again Dr. So Roy, thank you for understanding that when you have so many heads of the state in that 5 to 7 square kilometer area uh you know movement does become a little challenged but uh nevertheless uh I think we're trying to deal with three topics at the same time. Number one is AI, number two is trust and number three in my opinion is innovation. So instead of trying to give one common answer for me it is important is what are we who is our end customer what are we trying to solve because if we know the outcome then we will work accordingly. So for example, if my outcome expected is that I am going to do citizen services but I am going to do it in a secure way and I am going to build a trust that my data is safe, my money is safe and so on and so forth. Right? Now fundamentally it means is that first is building the trust. When you go to an Amazon or a flip card or when you go to any of the sites that you trust, you don't mind having your credit card information or your home address or personal information. At the same world, there are people who are constantly fishing. So my fundamental part is that is number one. Number two is everybody has tried to innovate so that we are able to remove the elements which try and hurt you. Now you know it could be by making sure that every IoT sensor has some security platform. uh I think it will take a lot longer than what we have currently built and we need to now innovate. We all know the end goal. The end goal is it's not only my phone needs to be secure. Every connected device is a potential where somebody can hack into my systems. Now for that to happen I think we need to make sure that while we put guardrails we put security around some of the key elements but we know that there are this is a psychological warfare this is cyber terrorism or cyber security challenges and uh the third thing is we need to prepare that if things go down can I recover fast enough. So I think I can only tell you that uh the company that I founded called Ionos. The first product that I'm launching is Uni Protect. Uni protect means if you work with me, I will be able to protect your identity. I'll be able to protect your data and I will be able to protect you against the terrorist attacks. and if the terrorist attacks do happen, I'll be able to recover it fast for you. So, I think it's a complex subject and coming in last and taking most of the time is not fair. Dr. uh uh Oroy, >> thank you. Thank you, sir. Your words of wisdom are always welcome. Now since the time is up I am reducing my 30 secondond question to one word and I'll request all the panelists to name one word which must exist for a citizen to trust an AI system. Dr. Ajit what is that in one word? >> Uh in my opinion it's transparency. >> Thank you. Uh uh accessibility and trust. I think that's two. But accessibility. >> Thank you. >> I'd say confidence. >> Yeah. I think it's ethical and public. >> I'd say it's uh privacy preserving. >> Thank you. >> Governance. Thank you. Thank you. And I think that was a very apt summarization of the panel discussion. Thank you everyone for such a lively discussion. And with that we come to the end of this panel. Thank you. I would request our speakers to kindly remain on the dis for a small felicitation ceremony. Please sir, I would now request CA Nero, former CMD National Fertilizer Limited and member executive council DCTC to kindly come on the stage. Ma'am Ma'am I request you to kindly felicitate Miss Jaru Shinasan with a shawl. I request Dr. Monisha. Ma'am, >> ma'am, >> thank you. [applause] >> Mr. CP Gunani sir Dr. Mad Noy will be easier for everybody to walk. >> I now request Dr. Sep Marva, founder, Noa film city, president AFT and member executive council GCTC to please come on the stage and felicitate our speakers. Mr. Ajit Kumar. >> Hi sir, how are you? >> Hi sir, how are you? >> Mr. Kumar Sharma say Mr. Abhinavali sir I would request our panels to please remain on the stage because the fellow station has not ended yet. Please we have a very small another token of appreciation for all our panel members. I thank you Sandep sir. Thank you so much. I request Mr. Adita Tiku convenor founder GCTC to please kindly come on the dis and honor our speaker with a planter. Charu ma'am >> Dr. Manisha over CP Gunani sir CP Gunani sir sir CP sir welcome a lot of exercise >> Dr. Madan noy sir you did it. [laughter] >> I now request Rajesh Kumar GI director shadow emphasis to kindly do the honors. Mr. Rajit Kumar Adjit sa Alkesh Kumar Sharmas please please please [applause] and Mr. Abhinavalini sir Thank you our esteemed speakers and our audience. Thank you very much. >> Thank you very much to join this session. We have only limited time to start session now for >> please be quick. We have >> 5:30 then Thank you to all eminent speakers. So I request to all the attendees and speakers to quickly clear the space so that other participants can join.