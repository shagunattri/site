Okay. And for people from different different domains who are experts in let's say marketing or healthcare or um teaching or finance now since everyone can build uh their own tools and platforms that they want. Autonomy is the new competitive advantage that is coming into the play and that is why so many startups are being founded every day. But since many of them are missing that wow factor to make sure that they actually scale, we have built an intelligence layer which helps you identify your uh code your market fit of the product and several other things which can help you identify if that product is worth scaling or not. Now before we talk more about how the uh what the entire tech stack and the architectural diagram of the product looks like. So one second. Okay, I'll just give a little more brief of what hacksale is all about. So as I mentioned we power innovation and product uh adoption in partnership with different corporate and government bodies. In 2025, we also created uh we also created a world Guinness World Record hackathon in Agentic AI with most number of participants and uh they built over 700 prototypes and deployed on GCP in under 30 hours in Bangalore and we are also powering the world's largest hackathon which is smart India hackathon with AICT ministry of education. Now to talk more about our product and how it is built and what all technology and architecture is there. Can we please have Atul you on the stage? Hello. So my name is Atul Bharaj. Uh and I'm here to talk about the product itself. Um uh I'm from a tier three college and one of the problems that I faced back in the days when I was in college was uh I had so many thoughts so many things that I wanted to build along with my friends but one thing which was a bottleneck for me was actually evaluating whether what I'm thinking or what I'm trying to build is something which is market ready or something that will actually work. uh along with this uh if I end up building a product, I was not sure if this is something that I can take to market. Is this is the code ready for production? Is it production ready? Uh do I have any security issues? Do I can I scale this up further? Is my code actually production worthy? Uh so um one of the things that I always looked forward to was a mentor, a guide who can tell me if what I have built has actually good code quality. Does it contains every security check that I need or the logic itself is it flawed? Apart from this, if I can have a strategic feedback that can actually help me to go to the market and you know sell my product. Um for this uh so we face this that uh a lot of developers that uh are a hack to skill or a part of our community also facing the same issue. For this uh we have created a agent or uh workflow that can actually help them uh identify the issues. Um so for our product uh you can always come to us and uh uh it has over 20 different uh language support. So if I'm building a product I'm not sure if it actually performs I can come here I can I can submit that submit here for evaluation. We have over 20 uh we have uh support for over 20 different frameworks uh for React, Python, uh uh uh other languages like Rust, Go, anything like that. What we do is we evaluate those on AI with the help of AI and tell you if your code actually lacks some uh important benchmarks um if it is up to industry standards or not, if your idea is actually worth taking to market or not. uh along with this at the end we also give uh feedback the feedback that you can actually implement in your product and scale it up accordingly. So the process flow is like that uh we have a web- based application. Uh the user actually comes on the platform and gives the uh solution uh that can be built on any product. Uh we do the analysis and then uh do the respective scoring. Now this scoring actually helps you to actually further improve your product. Uh we have a small demo how the product works. So uh here you can see this is our uh platform where you can come here and uh we have provided a GitHub URL. We'll be fetching this code and we'll be benchmarking it on various parameters. At the end, you'll have a complete in-depth analysis along with some tools uh along with some tips that can actually help you improve. Now I'll uh ask to actually guide you through how the evaluation platform works. Yeah. So what is the brain and heart of this uh problem that how we are actually solving this since there are any number of parameters in the market on what benchmarks you are actually testing. So here we actually introduced uh the multi- aent analysis system where user uh like gave a number of parameters for an example you are facing a problem with code analysis uh your you have security concerns you have testing concerns that they are actually working or or not. So what we are actually doing is we uh actually developed a dynamic agent system which particularly work on a particular uh parameter only. For an example, I wanted to test uh the efficiency of a code. So that particular agent will learn on that only and after that every agent has done their work and all the engineering part is done. We have a master agent what actually just compile all the code that actually we are solving this problem. are agents working in the right way or not and they compile all the agents uh in a particular format and we just compile all the data and showcase it to user that actually what they have to improve in their code or in their problem solutions. Yeah. So submission as as you can uh see in the video that uh we get submissions in multiple formats also. There can be code, there can be PPT, there can be videos, presentation and documents. So whatever we get to know like the project submission type is on video or PPT. So we can't run code analysis on basis of that. So what what we do we dynamically define the parameters that is the solution is really efficient or not in the market. They have really business solution in the market. What are the scalability criteria within that? So we also define the those parameters on basis of the submissions and uh the results are uh get compiled on basis of that only. Yeah. So these are some uh dynamic parameters which we use. They can be uh like anything whatever the user demands. Yeah. So this is the flow where each agent get a task that what they have to do. Since we are in the era of AI, so we do talk about every agent is a engineer, every agent is a worker. So this is the flow that we really actually work on that that uh every agent has to a particular task and has to perform a defined set of rules. Yeah. So at can you please mention how we can scale it more uh within the architecture side? Um so what we have discussed so far is how a single innovator can come on our platform. They can test their own idea and the product itself. Uh but how does this help enterprises? Uh let's say you have a workforce of thousand employees and uh you organize a hackathon uh innovation uh challenge series where you can actually crowdsource different ideas. How this help is uh instead of evaluating manually evaluating thousand different submissions on your own you can use our platform where you'll be evaluating more than hund thousands of solution within ours and uh u apart from actually crowdsourcing innovation innovation or ideas you can also uh use it for uh product placement u you have thousand different users that can actually use your product give the feedback on that using this innovation management. Apart from this u let's say if you have different different employees coming to you and they have built their own side projects. This also helps you to vet if their idea can also be taken to the market uh as a as a project uh in itself. Uh the very last thing is we have a seamless integration where you can actually help uh where you can can uh get the solutions or the market readiness uh to your uh uh different platforms. For example, you can get your results on discord, slack, um you can have other CI/CD uh integrations as well. Now this helps an enterprise reducing the evaluation time up to 90%. U a general a generic evaluation for any platform might take anywhere between 30 minutes to an hour or more than that but this takes only 3 minutes 3 to 5 minutes and this significantly decreases the evaluation time itself. The next aspect is the technical depth. Not everyone is capable of evaluating these text stacks or the problem statements that you actually uh build. Not everybody is capable of uh evaluating those ideas. This helps you cut down on that as well. The last thing is innovation. You get the cream layer because this helps you to evaluate who are the innovators in your own company that are actually contributing. Having a bulk evaluation will always give you uh enough time to cut down u and give a enhanced comprehensive detailed report that can actually help you grow. That's it. Uh we are open for questions if anybody have any questions. Uh amazing product I must congratulate. But uh the one concern that comes to my mind as if as a as an architect developer the design architecture which I have in my mind and let's assume I have done this with some agentic AI and something now when I'm sharing my thoughts and the entire architectural data sets and the design thinking on your platform how about the integrity of my system it first You understand? If I am putting it into the system while I am privy to the knowledge of my so-called uh great business idea but it is reaching beyond my thing and I am putting it with you. How are you ensuring the integrity of that system? Yeah, as I have already mentioned that uh we do define a certain parameters just just like mentioned you have a problem that uh a crop will grow or not right so you have a solution with that to you have to define the benchmarks for that that these are the certain benchmarks you have to first notify me if a crop is getting damaged or not you have to define me that what season I am running into is there any uh any any seasonal uh rain or any you can say disturb in the weather. So it should be notified mean you have to define that benchmarks first. I'm talking more about in terms of the IP and the security of my business idea. Let's say I'm sharing and see one who is processing my uh on on your company behalf. Now in the in the process of working and doing all those iterative uh work at your end, she thinks this is something great and I must be doing this and she picks picks up that idea and start doing before I start doing it. My IP and my business is out of my hand. How are you ensuring that my idea will remain with me not with somebody else? I I I hope I am able to uh frame my question. >> Yes sir. >> So yeah that's a valid concern. Now u as far as AI evaluation is concerned uh we have uh uh a separate uh u uh we have separate instances uh where your uh code and everything is evaluated in an uh isolation and uh we actually uh dump it down once we are done with your evaluation. That's that's part one. Uh the second part is uh if you the security part uh where your you uh yourself are having the IP or not. So yes um u so your idea is with you uh it is not displayed anywhere to the end uh to the public uh it remains with you and uh it's just accessible by your your own self. Any other questions? All right then. Uh, thank you so much.