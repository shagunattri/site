that is happening as much as you we are talking about it in India and that experience is what relates to the kind of performance that uh you are seeing on the numbers especially when we have grown from maybe about $4 billion uh four five years back to about $22 billion uh in in the recent year uh almost maybe you know 55% to about 70% growth that we have seen that said uh we continue to witness that kind of a momentum worldwide growing going up to about what $40 billion. That's the kind kind of guidance that we have given to the market. But most important what you also would like to know is the fact that you know all of this is delivered by technology which is uh developed in-house end to end. And it's important because uh when you're looking at any kind of a data center setup, you could possibly have one partner trying to uh bring all of the technology and lead that technology but also maybe in situations uh there are close to about 20 partners or 30 partners and 40 partners and that also gives uh the segue into how we are differentiated and what do we do uh in order to bring the best. uh one of the other points that you will also relate here is the fact that you know uh we have manufacturing facilities world over when I say that primarily in three locations uh we pretty much got uh US uh Netherlands uh Taiwan and Malaysia all put together with a capacity close to what maybe about $70 billion uh to hundred billion dollars kind of uh you know production that is uh capable at this point of time and obviously we continue to go. Uh also the fact that um when you look at all the gigawatt kind of announcement data centers announcement that you have been witnessing even in this country and I'm sure by 2030 you will see about you know 7 to 8 or maybe 10 billion 10 gawatt capacity of data centers being there. uh the only uh company which possibly has a reference of a 1 gawatt uh factory or 1 gawatt data center being set up is super micro. Of course, you'll get to hear more of how we've done it, what has powered it, what is the experiences that we have had from some of our colleagues uh both uh uh from India as well as uh uh US. We continue to see this uh not just uh worldwide but also in India as you would have seen maybe with that we anticipate close to uh you know 200% kind of a growth in uh some point of time I think we uh we have spoken about uh uh the installed capacity or the kind of racks number of racks maybe uh one of the important point that you would see here is our ability to sort of produce close to 6,000 racks uh a monthly rack capacity and when you talk about liquid cooling maybe about 3,000 racks and in fact uh when you associate that with the kind of uh power consumption that we ourselves have from production maybe that is only limiting factor but also the fact that uh I don't think many of those factories would even have that kind of a power to just produce we are talking about 63 megawatt of power that we currently consume in producing those uh uh servers, racks and uh data center infrastructure. Uh most important when I mentioned that you know uh we've been in the business for close to about two 20 years you would have seen uh some of our partners uh present super micro solutions and some of the partners that will continue to be part of this journey are highlighted in this. I'm sure there are many of you representing those companies in this forum. So many thanks for uh supporting us in the past but also supporting us uh have the vision for the future uh for the next uh four five to six years as we in India want to develop this. What you would want to also know us for in the context of uh uh you know the growth that we are seeing worldwide uh is our ability to differentiate which I just spoke about and that differentiation comes from uh all kinds of uh you know uh all kinds of products and abilities that we have demonstrated especially the data center building block solution that I wanted to highlight. uh we are possibly the only one uh which can uh bring in right from a server to a rack to CDUs, manifolds you know um even the chillers that are required and hence end to end capabilities both from a design but also from a perspective of producing it ourselves without having dependency on any other assembly operations worldwide and that itself gives us the power to adopt the latest that is coming from the semiconductor world. Many of you would be able to relate that it's no longer the semi semiconductor life cycle or the cycles used to possibly be about year or 18 months whenever they used to launch one product. Right now we are we are in an a world where there is a launch maybe once in 6 months or once in 8 months or maybe once in 9 months. Right? What it means is uh any server company, any AI infrastructure company will have to have that control on the ecosystem, the deep engineering capability that it has in order to quickly turn around that generation of semiconductor and the ecosystem uh innovation that is bringing and in order to then turn around produce those servers end toend capabilities, it has to have that control. So we uh are possibly the only one which has that kind of a capability at this point of time and uh you will hear more from our colleagues. So with that uh I would um welcome uh my uh uh senior colleague Wick Malala functions as the president for IMIA but also the senior vice president technology and uh uh especially AI uh to talk about the perspectives that we have seen globally and uh how are we dealing with these perspectives both in India and uh worldwide. So thank you Vick and uh a warm welcome So one thing we realize is that we don't know how to put the slide decks right. So for people who um haven't seen it, it's basically about accelerating the data center infrastructure, right? So um before that I just have a question. Um how many of you are involved directly or indirectly on the server hardware? Very few. Thank you. And for the rest of you, I'm going to bore the hell out of you in the next few minutes. uh because we are going to talk about what is happening with respect to the data centers and what is happening to the server infrastructure and uh what is that we need to do and what are the pitfalls and how do we approach that problem together and what super micro is doing to address this so that you know we can be relevant in this market. Okay. Um so first of all when you're when you're starting to take a look at the data centers especially now the biggest headache is the spend. You're talking about huge investments. You're talking trillions of dollars in terms of in you know investments for the data centers. What does it mean? You take a look at any kind of deployments today historically if people are talking about 20 30 megawatts of data centers that used to be a big deal but nowadays with the AI infrastructure the spend is becoming bigger and bigger so when people are spending that kind of money there is a cost of money people need to realize an output from that you know when they are putting so much money on something they have to recover their expenses that's the first part Second of it is the same amount of money if much of it is actually used in creating the compute demand. That is a better way of putting the money. And every one of us that are involved in the data centers in some shape and form know the biggest is not actually the money that's a problem. It's the available power that's the biggest issue. If you take a look at India today, the amount of power that is available for the data centers in the whole India is around 1.4 gawatt, right? You can multiply with hours and come come up with a bigger number to make us feel happy. But the reality is that is the power that we have. The good news is that there is a significant investment happening from different industries and the government is supporting it which actually will get us close to like you know seven or eight gawatt of power in the next three or four years. This is incredible if you think about the amount of uh effort that goes in in building a data center and what is happening in that that actually quite excites all of us right. Um so then now it's so difficult and much of it much of the power is coming from non-renewable energy sources but Indian government is actually pushing hard for at least certain percentage of it need to be based on renewable energies some of these companies are making based on hydro some of them are based on uh solar and some of it is wind and how to ensure the baseline is established at a way where the usage of uh fossil fuels is limited on that one. So when you when you when you say for example you need money now you have power. So when you do both of them put together you put infrastructure in it the first thing that everybody starts looking at is when and how am I going to recover this money. This is the return on investment that everyone talks about bottom line what it means is that you need to be looking at optimizing things at every aspect of it so the money is put to good use and be able to recover that money. Let's say for example they have all these infrastructure companies like super micro provides that they will put it in a data center get the power starts running everything is fine who is going to use that this is where the offtakers are today right so we need to start looking at the applications that are going to come and how these applications are going to make people you know their life better this is where many of the people that are here in the room that are involved in some shape and form on the software becomes very relevant because this is the one that's going to determine all the infrastructure that's being put together is put to good use. Right? So looking at this equation, that's where the AI market is. Lots of money, lots of power required, optimized solutions are needed, the developers becoming an important play in this entire equation so that we can have a better result. Now what's happening with that? So we put all this infrastructure. Why do we need this large infrastructure? mainly because I mean I'm sure every one of us in some in some way using chat GPD for example right or Gemini or any of these uh gen AI tools if you take a look at the amount of compute that is needed to train these models that's incredible and part of it is related to the amount of data and second is how long it takes to converge these models to have a meaningful u you know model coming out of it and what does it take to continuously improve and tune this. So if you if you start taking a look at it um what used to be like a one one and a half trillion parameters that was like a big deal but now the latest one if you're talking which is um I think it's um uh the new gro model it's like a six trillion parameters right and looking ahead it could be 10 or even higher trillions of parameters these models need to fit in the memory of these clusters of GPUs that is what requiring a larger and larger GPU clusters that is required to train these models. The second of it is these GPUs can be sitting in isolated pieces or they can be connected using a very high-speed low latency interconnect fabrics. This is where uh the difficulty comes in when you are putting all these GPUs and you need to figure out a way to cool them. This is where the liquid cooling is coming into the equation because normally what happens you need to have some kind of fans that are blowing air over these components to take the heat out so it will be staying operational. Other way is basically put some kind of uh liquid cooling. I'm sorry I'm blocking the view perhaps. So if if you start taking a look at you know what does it take uh for these things to be cooled. This is liquid cooled and the power delivery point of view. Historically, you just take it and connect it to a wall outlet or maybe a 220 volts or something like that, the power supplies. But the larger ones require power delivery using the DC bus bar. So, it's another thing that is changing. So, we are talking about components that are running very hot like 1400 watts and even 2,000 plus watts very soon. And you're talking about rack level power that is required that is much larger than traditionally what we are used to and be able to cool all this thing in a better manner. This actually means that when you're building a mega scale data center, you start looking at things differently. Gone are those days where you say, "Oh, I have this data center. It's been 20 years old. My friend, it's not going to work." So, either need to be retrofitted or you need to build something brand new. But the problem here is if I start building a data center today, it may take a year and a half, two years to build it. By the time the GPU kind of the whole uh product profile is changing so much it already becoming outdated by the time you finish that construction. This is a big big big problem. So what people are looking at it is how do you make this modular? How do you ensure that these are able to be retrofitted or be able to modified at the time of something to be rolled in. So today for example um let let's see for example this is an example of what is happening um pandemic like in a 2021 time frame until now you take a look at it like four to five years what used to be around 500 watts for a GPU or a CPU it changed very quickly what is going to be deployed in the very near future like a Vera Rubin I'm sure you heard of it when you're looking at Nvidia GPUs or um AMD based Helios I saw that in their booth a big rack right you're talking about 270 kilowatt on the uh rack level but at the same time if you take a look at a GPU it's over 2,000 W these are liquid cooled and whatnot but if you have a data center that need to accommodate it traditional data center has no way of doing it luckily in India as I mentioned much of the data center buildout is happening brand new and this is the reason it becomes very important for us to understand what's happening on that front that way we can build it properly deploy it clearly. Other thing what happens is traditional data centers may do 2 to 3,000 pounds which is like500ish kilos of uh rack. Beyond that if you start rolling on that it may actually break the tile. Some of these systems are actually going to 7,000 lb. So you can imagine a different level of um I would say data center buildouts are needed. So we need to take that into consideration. If you take a look at the systems, this is how it pretty much looking like you know you can see the red line which actually going to like a 250 260 kilow per rack per rack a single system when I say rack is not even like you know 20 30 whatever systems all these are connected using fabric this is a single system that's consuming like 260 kilowatts if you think that is actually impressive just wait for one one and a half years we're talking about one megawatt per rack Google has already announced that with the TPUs they're talking about a you know megawatt per rack how are we going to cool that how are we going to give it power all these things so when I talk about it we start looking at things beyond what happening at a server level we are looking at in a holistic way when they start getting deployed in a larger scale what is that we should be looking at because think about it we as the partners with Nvidia AMD Intel and several others we have access to technology and based on that we can start building but the data centers their job is different I mean they could be coming from real estate they could be coming from um let's say cryptocurrencies where they made money and whatnot they're kind of transforming that into data centers so how do we bridge that how do we enable that so if you take a look at now these kind of products the the one on the left side is where we have the GPU platforms but they need to be cooled in a different way in a traditional data center you probably use something like uh liquid to air which is uh think Think of like a big radiator sitting next to the rack of servers, right? versus the ones that are actually using liquid um basically things coming to a chiller where we can actually use uh some kind of an inrak or in row CDU sorry you know the heating distribution units right cold so then what happens is that um you'll be able to handle the systems um all the cooling requirements in that but then on the far right is what we have these are called the water towers or chillers right uh besides the hwack these these are things that are needed did and let's say for example uh I was born in Hyderabad right so in Hyderabad you know we don't have as much water if I put these chillers and then I start running water the first thing they are going to do is kill me right I mean we don't even have water to drink and now we are talking about these so which is not good so we need to have dry chillers for that so depending on where we are in the part of the world we need to look at what are the requirements and based on it's a dry cooler or a wet cooler or a hybrid manner and be able to you know roll that in the data center and okay now we talk about all these things but what are these systems looking like so typically what we are used to is the rack mount systems whether it's Intel based or AMD based you know a 1U or a 2U type of form factor these are form factors right and the second thing is related to multi-node most of the people that are involved in the high performance compute know this the dense compute with uh highly connected internet interconnect fabric when you put it together these are the ones that are being used then the GPU servers. I'll have my colleague to go into details on you know what all the things we are doing on a GPU side but typically a PCIe based accelerator or a highly interconnected ones with like NVL link or OAM form factor. Uh these are the type of GPUs that are being used and then on the far right what you have is basically racks of systems where the entire rack is a single system. You may have seen something called Grace with Blackwell like a GB300. This is one type of thing and Helios though the two that you can think of in those lines. All these things are need to be connected with u a good amount of storage because it's impossible uh to handle these large models without having enough high performance storage that is closely connected to the uh compute. So this is basically the product portfolio how it's going to look like that are going to go in a data center. Now what does it mean? So we need to start looking at u you know systems that are at a uh system level and from a system level now we're starting to look at the systems at a rack level when multiple racks are put together using fabric and these things are connected you know uh to comp other storage and other infrastructure within a data center and then it's connected to chillers or whatever outside think of it in a holistic way right but now when you connect these many systems how do you ensure how do you manage them how do to provision them. This is a big big big headache, right? For a compute infrastructure point of view, many people that have been handling this for a long time, they know. But now we are talking about Neoclouds. We're talking about the companies that are never ever touched the data centers are getting into this business. So it's important for us to provide them the tools to manage these infrastructure in a you know eloquent manner. Right? So that's basically what we are trying to do on that front. So this is what we call it like a data center building block solutions. So as I mentioned the systems when you have especially um you know if a single system you take it you have a processor and a CPU and you have um GPUs and we have um some kind of VRM and then you have add-on cards which could be uh either Ethernet or Infiniband you have storage controllers everything will have its own firmware and you have BIOS all these things need to be updated periodically to get either maximum performance or in cases where uh there could be security flaws that they need to be updated or patched. So when you're talking about one or two systems is easy. Now we are talking about doing hundreds or even thousands of systems especially when you're talking about deployments that are happening at a giga scale. So this is the reason we started investing quite a bit on a system management tools be able to manage and provision as well as updating this kind of u you know infrastructure quickly. So then now I mentioned about system management but now we're talking about data center level. So how do we do that? So at the very base level, if you think about the racks at the bottom, the easiest is to talk about the system administrators that have access to these tools to go and manage the systems. But then when you're talking about multiple systems in a rack or in a data center, how do we handle that is a data center operator headache, right? But then many of you coming from the software side now, you know it is going to be about the developers and how they can have access to these systems. So it's about the DevOps is one of them and it's also about providing tools you know for for the developers u when they're developing something uh natively on the cloud. Um so then this is more of like you know just giving you an idea of what are the other things that one should be considering global services um things will break especially when you are using at a you know I don't know how many of you are from hardware engineering but things will fail at a GPU level or interconnect level there could be signal integrity issues there could be mechanical issues there could be so many things so obviously the telemetry and diagnostics is part of it but when things fail how quickly are we going to be able to fix it. Let's take an example of a GB300. We're talking about roughly $4.5 million per system. If it goes down, $4.5 million is not being in production, which is not good. So, we need to be able to very quickly bring it up. This is the reason on-site services come into picture. One can make an assessment to see what it is and we'll be able to either support directly or through our partners. At the same time, when you are bringing these kind of clusters, people haven't deployed them at scale. So I think um um Suresh mentioned in the beginning that we are one of those very few if not the only one as an OEM have deployed a gigascale data center and the public information is that we actually are the ones that enabled uh Elon Musk with the XAI right so this is the reason it's it's not easy thing there are a lot of things that go wrong but we have learned you know what to do what not to do this is the reason we want we we're saying like okay maybe we should start helping our customers in the deployment point of view this is where the deployment government services will come into picture. Again, India being one of the markets where we heavily work with our partners, we'll be either using our own resources or our partner resources to go do that. And then this actually provides a way to optimize the uh clusters and bring the right performance to that. Now I mean just to kind of give you a summary of what's happening typical data center. So you know you have some kind of transmitters and then you can get the power into that you know local transformers that is providing power this the infrastructure space that's happening you have existing cooling as well as you are able to bring water towers and all that to make it. So what is happening on that the first thing is in a data center you have right now you have compute and you have storage you have networking switches you need to find a way to cool it down at the same time the cluster and cluster management. So this is what basically goes within the data center. So what happens? You need to enable it with power. The power has to come without nothing works. What the output? Number one, heat. That's not something we want. We want to reduce it. The second is data. In a AI data center world, it's nothing but tokenized, right? You're talking about I give you electrons, you give me tokens. That basically becomes an you know AI factory. So how we are actually looking at it is in each of these things what we are going to enable our customers with. So let's take an example in this case you need to have power. Power could be coming from more than one type of sources and we need to be able to accommodate that and we need to be able to roll it into our management tools so we can still provide a single pane of glass. Similarly, you take a look at on the cooling side, you have either dry cooler or a mechanical or water tower, whatever it may be. And we need to be able to put a way to put the software that is managing it. And we need to be able to give help in developing these data centers from ground up and as well as help with the customers on building these complex clusters with a cabling and you know telemetry associated with it to handle it and the data center build happens. Will all of this can be done by a company like single company like super micro? The answer is no. Right? This is where the partners and the partner ecosystem comes into picture. At every one of these stages, there are more than one supplier. It could be super micro, it could be one of the partners. Our job is to make sure that we understand the end to end how it looks and what is involved and see who are the major players in that. That way when we enable it and when we helping customers parts of the building blocks will come into picture from super micro parts could be coming from partners. That's number one. Second is when we are putting it all together how do we ensure that it is going to work. That's where a bit of uh work that needs to be done from our friend. By doing all that we are able to enable the data center from an empty shell into something of an AI factory that is going to provide tokens in a most optimal way. This is the effort that we are putting. So beyond a server, how do we enable data centers especially in a country like India where we are talking about massive massive investments and much of it will be borrowed as a money and how do we enable these guys better? This is the idea for that and u that is part of my presentation. But in terms of the products what is actually enabling us to do that I'm bringing Aloc Shivatu who is actually a product manager that is focusing on GPU products and AI products. So he will go into the details. Thank you very much. Thanks Vic and thanks Surish. So to begin u to begin with Vic as uh pointed out this one that we are building AF factories now and uh my name is Aluk Shawastav. I am uh director AI infrastructure super macro. So you know India right now we are talking about a new chapter that is democratizing AI and u it's a AI impact summit 2026 the first in the global south and uh the theme is really wonderful and very noble theme that is welfare for all happiness for all based on three major pillars the people planet and the progress. So when we talk about progress, everybody knows that the most important driving factor for this progress is going to be AI. And when we talk about AI, so in last few years, significant improvement have been made in this particular field especially you know it's sort of a paradigm shift and when we talk about paradigm shift we are talking about multiple inflection points. So the very first one was just you know few years back that was um um in fact you know the exact date is 22nd of November 2022 the the date um when openi they introd introduced jet GPT and that that day is pretty much you know considered as the watershed moment in the field of artificial intelligence because that was the day everybody realized what the power of AI is and what it what it brings to the table. The next one is the year 2026. The most of the market research it indicates that this year 2026 will be marked as the year of a scale. When we talk about year of scale, the the reason is very simple that now AI is moving from experimental tool to infrastructure utilities. You know what Vick was also mentioning about the AI factories. So the most important you know factor is inference because this is one of the most important workload because in inference is where um the end user meets the application and at super micro we are building these AI factories for this industrial era with full responsibility and um energy aware AI. So how we do that? So you all know that um you all might be aware that at super micro we have one of the largest server portfolio in the industry and these are you know some of the product families what you know Vick also mentioned about this one and under each product family we have multiple options. So we have multiple options under each product family and especially today you know we are going to talk about the GPU servers because this plays a very important role vital role especially for for for these AI workloads broadly it can be classified into three major categories. The first one is HGX PCI and MGX. The HGX1 is playing very important role. Whatever you see AI is doing right now, the HGX based platforms, the OM based platforms, these systems played a very important role. The PCI based systems, these are, you know, pretty much you know very flexible systems. These uh multiple different type of accelerators can be used on these platforms. And last but not least is the MGX. We work very closely with Nvidia. So these are the systems especially designed for for for the accelerators that is based on Nvidia. So how we do that because we always claim that you know we have one of the largest EAP server portfolio in the industry. So the reason is very simple. The answer is very simple that this is based on our design fundamentals and super micro's design fundamentals are that it is based on building block architecture and because of building block architecture we can just you know configure the systems in different ways and that's how we come up with multiple options not just to confuse customer but to provide them the exact exact you know solution for their specific needs. So real quick on this one. So super micro you have seen you know Vick also talked about that one and earlier also that we have a width a huge range of these systems but all these systems you know what we are talking about is specific to AI we are very deep rooted in this particular um in this particular type of infrastructure right starting from the first one the very first one that you see on the screen that is DGX DJX1 and the DJX1 is the system that Jensen so many times you know you might have heard he mentioned about this particular system that this is the first system he delivered to um to open AI. So this is the very first system from where this you know all these the major changes that you see in the field that that started and followed by you know the A100s the the B B100's B200s and finally the B200 that you see at the bottom and the performance difference you can see it is started with one petlop and as of today the most advanced chip that is based on this the the B200 blackwell that is that can generate up to 144 petlops of compute So humongous change, humongous you know uh improvement in terms of compute capabilities. So that brings us to this the the next one that one of the most advanced chip right now available in the market that is blackwell the B200s B300s and there is a significant shift between B200 to B300 and B300 you can say is the pretty much the most fundamental or the purpose-built systems especially designed for this type of workloads what we are talking about AI factories everybody is talking about AI factories so this is this is the fundamental system from where this journey really starts So the timing is perfect. These systems are purpose-built systems especially for AI factories. The one that you see at the bottom the B300 based platforms. So here you can see the we will talk a little bit more about the HGX based platforms. The these platforms are very important because these u these platforms are the fundamental building blocks as we mentioned earlier and all the data centers that Vic was Vic also mentioned that pretty much you know all data centers they are power limited and order to get the best TCO out of it. It is very important to to use that power envelope in the most efficient way and the one that you see on the screen this particular system is based on our liquid cooling technology that is you know DLC2. So before this the this system is successor of the previous generations the hoppers you know what we were talking about earlier the H100s and H200 we will we will you know uh disclose a little bit more information on that one that how we really really did well and we created a very good um you know uh example in the industry that we did that those deployments at a very very fast pace. So this system is a next level. It's improved a lot from the previous generation which we already did the hoppers and here this is DLC2. So the key difference is that earlier it used to be only the CPUs and the GPUs those were liquid cooled but this particular platform you will see that it is not simply the CPUs and the GPUs but also the DMs the VRM even the power supply is liquid cool. So these systems they finally you know at super micro we always provide that um them in full rack stack cable label solutions. So you can see on on this particular there is a rack most optimized one it is going to be eight but it can go beyond that also less than um num number eight but it finally the most important thing is it depends how you know your your workload is how the design is but this particular architecture that you see is the most optimized one. So in terms of performance you know what we were talking about earlier that is capable of generating 144 paflops of compute power and same thing applies to this one. This is a 2U system. So this particular system we recently introduced at OCP just few months back and this system is gaining lots and lots of you know momentum lots and lots of interest in this particular system. The reason is simple because with this system you can if the the given that you have power available in your data center you can pack the maximum density in in a at a rack level and what we mean by that. So you can see here you can put up to 72 GPUs in a rack which is very similar to the most important and the most you know the famous architecture that everybody is aware GB 2000 GB 300 NVL 72s. So here you can see that okay this system in with this architecture also you can put 72 of these GPUs in a rack. The key difference is this is for a scale out and the GBs and all this stuff what we see that is for um a scale up architecture. So that is the key difference and lots and lots of customers they are still on x86 architecture and want to capitalize um you know this this much compute density they can definitely you know go with this uh this particular architecture. One very important difference is this is not 2 this is 2. So here the width is different instead of 19 in it is 21 in and it is you know especially for u u it is based on ov3 rack design and all this stuff it follows that and the key difference is that t is increasing as well because you see that lots and lots of it is we are talking about the scale so it is percolating from hyperscalers to neoclouds and from neoclouds even to enterprises. So in you know in our customer base there are customers there are some customers that is basically they are um enterprise customers but they are also looking for this particular this particular architecture. The reason is simple because their data centers are new and they can they want maximum density. So how you can do that? So you can simply double this density. So you can see in a rack at super micro we are we were the first one and that's why you know we were getting very you know lots and lots of traction at OCP also because we can pack up to 144 GPUs in a rack. So in terms of compute capability it can generate up to 2.6 xoflops of compute power just in a rack. So the next one is you know as you see that lots and lots of data centers still even today even in US across the globe they don't have liquid cooling capability available so they still go with aircooled versions. So this is the system the aircooled version and in terms of compute capability again it is same 144 paflops of compute power on on this particular platform also and this is uh this particular system this architecture is based on our previous you know generations of our flagship products starting from the the A100s the H100 H200s and so on. So the next one you can see that again the most optimized configuration because of power and all this stuff it is going to be four systems in a rack and this can be up to 60 kilowatts just just at a rack level. So the next one um you all might have heard of this one the GB300 NVL72s lots and lots of buzz around this because of because of these the the the latest workloads and the key here is that this particular system is very complex very complicated and at super micro once again we are working on some of the world's largest deployments that that is under process right now and pretty soon we will be able to share with you few more success stories based on this particular platform because this is very complex system and because of you know super micro's past experience we could make it happen that we are again doing this deployment in a record time so you can see the the gradation you know how it started earlier it used to be just you know 20 to 25 kilow racks and now you can see at what pace you know this this um the the rack capacity capacity in terms of you know the how much compute capability you can pack in in Iraq and where it is going. So where we were where we are and where we are going. So this is a clear example. So here as of today it's it shows the maximum is 227 kilow but still you know there are some designs we are already working on that that will go up to you know 400 600 and even up to um up to 1 megawatt at a rack level. So this is just to give you an idea. So the systems earlier we were talking about then it goes into these racks clusters and finally these clusters just like you can see just this this um in this cluster there are so many the cables and all this stuff lots and lots of work and lots and lots of complication and lots and lots of expertise is needed to make it work and finally it goes to these you know the the data centers the multiple of these units just like we call it a SEO scalable unit and multiple of the these These scalable units goes into a data center and these data centers pretty much you know they are acting like AI factories and the most important you know part here is that all this stuff you know what you saw so far that uh you know is is needed and uh we we are we are building all of these type of deployments and the most important thing is the workload you know what Vic was talking about earlier the workload is changing also at a very very fast pace when we were talking about shared GPT and all this stuff. So it was just 1.75 billion parameter uh 175 billion parameter models and today we are talking about 10 trillion you know uh 6 trillion is already out and 10 trillion is coming out pretty soon. So lots and lots of compute is needed. whole data center acts as a building block. And so so we started you know with this concept that okay the whole data center is a uh building block solution and the the success story you know what Vick was talking about earlier it was not but now it is pretty much um um a known secret everybody knows about this one that the world's largest deployment based on liquid cooling was done by super macro it was 100,000 GPU um GPU cluster the first 50,000 were uh H100 based. The second 50,000 was uh H200 based. So the total 100,000 GPUs cluster was deployed just in 122 days and it was it was definitely you know we already mentioned so this was done for Elon Musk his his company XAI. Now you know so far you know all the the systems what we were talking about this is this is where we were where we are and where we are going now. So the next one is going to be Vera Rubin. Ver Rubin as you know already you know you might have heard a a lot um buzz about this one. So this is going to be even more complex more complicated system because lots and lots of changes already happened on this one as well. So first of all there are six changes you know all the major six chips they all are pretty much you know the latest ones. So all that is changed and the the most important thing is there is a architectural change also. Now we are talking about KV cache you might have heard of that one. So we are talking about this generative AI based uh inferencing that is not simply a oneshot inference what it used to be but it is not sort of a thinking process. So for that you need lots and lots of memory and for that the KV cache part is also involved in this one. So that is going to add more and more complexity to these type of architectures. The next one is very important. So far it is mostly about the compute the training and all this stuff. But AI is an incomplete without the visual computing. So visual computing is going to play a very important role vital role. without visual computing you know the this AI is not possible and uh the most important thing that is going to happen that the the next wave of AI that is going to disrupt the world's largest industry that is heavy industry so for heavy industry this visual computing is going to play a very important role and finally you all are you know hearing about this the the agentic AI and the the reasoning models the robotics and the physical workers and all So pretty much you know for for this next wave of AI things are already percolating to that level and it is going to be the big biggest disruptor in the industry for that again purpose-built systems are needed. So the one that you see on the screen this is one of the most advanced building block solution that is available for this visual computing these type of applications. This particular system can be powered by eight uh RTX Pro 6000s or H200 NV um NVL those type of GPUs. So this system is capable of generating the maximum compute capability at a node level. So earlier we also you know mentioned about this one that at super micro we are considering the whole data center as a building block. So if we are talking about whole data center as a building block. So it starts from the system level what we were talking about then it goes to rack level cluster level and super micro is the only vendor that gives you end toend solution and other vendors they also talk about end to end solution but our ends are different. Our definition of ends are different for us it is from cold plate to cooling tower. The one that you see on the extreme right. So from cold plate to cooling tower everything from super micro one throat to choke and this is little bit more details about you know what it what what is covered under DCBS. So all these the services the liquid cooling part all everything pretty much you know it's a body of work that what we do at super micro. So that pretty much you know encompasses in this the DCBS solution. So it's a framework basically it's not it is not just um it's it's a framework guide um designed without being prescriptive. So we are open generally we are you know working with customers as Vic mentioned they have money and they don't know exactly you know how to reach to this final thing. So super micro is working with those type of customers also and there are customers who know exactly what they're looking for and something in between. So we are working with a range of customers just you know from who needs handholding right from the beginning to the expertise you know they know what exactly they are doing they in fact you know tell us what exactly they want and we make it happen for them and in between also if there is any gaps in in in between that also we can provide so end to end at different levels without being prescriptive so that's what super micro is that's all from my Namaste. So suran cambies why don't you come upstairs? Let's uh let's ask some questions. So I know it's a lot to grasp right because especially for us we are living and breathing this. So we know but at the same time for many of you it may be new or maybe you know I don't know right but the idea again here is to take any questions that you may have if we know it's great if you don't at least we know what to work on but let's go with that and if you have any questions please bring it up and if there's another microphone that can be given he's bringing it so in the front row is yes go ahead Hello. Yeah. So as we know that in server side uh there is lot of latency due to IIO delays. So is supermarket working on that area. How we can uh reduce delays like for example PCIe. So they have their own delays. So some kind of future generations like photonix. I think there is research going on for photonix kind of interconnect. So super micro is there any any kind of work happening on that front? >> Um what is happening in the industry right now is that bringing the packaging where the the silicon photonics or you know even the connectivity directly to the chip which actually will improve the throughput as well as reducing the latency also reduce the heat that typically comes from the you know optical components right much of this actually happens in the silicon vendors. So what we are doing is to work with them right. So for example uh if you take a look at Nvidia Nvidia their GPUs are connected using uh uh NV link as a you know uh connecting fabric. The latest generation they're talking about 3 uh uh 6 terabytes per second is a throughput between these GPUs. I'm talking about Vera Rubin right. Similarly, if you take a look at uh AMD, they use the open accelerator module which is the OCP standard which they are connected using that fabric, right? Um you are absolutely right. If you take a look at the PCIe PCI gen 6 even gen 7 and when it when it comes um while the throughput is there the latency point of view it's still going to be a one of the sticklers but what is happening is that you need to look at in a slightly different way for training training models where there's lot of data movement that's happening between GPUs and across multiple systems or GPUs this is where the latency becomes much bigger of a headache because the model does not fit into a single GPU versus the train models if you're using for inferencing much of the data actually fits within the GPU main memory. So what Aloc showcased was like 288 gigs of memory that is sitting on a GPU. The model fits in the inference does not need to go and pull data out from a different GPU. So um short answer is super micro is involved indirectly by working with those silicon partners to bring the technology into our platforms. But other way to look at it is um it all depends on what application that we are trying to do and based on that which inter interconnect fabric that you need to take because mind you advanced technologies cost more money also we need to see how the ecosystem need to be fully utilized. Um do you do you have anything to add to this? So there are sorry so there are several ways of you know handling this type of um this type of um um challenges and one of them is the numerical formats. So you know to answer your question the new numerical format that is being introduced that is NVFP4. So with using NVFP4 so multiple you know the key reason is to reduce on the latency and make it happen you know faster. So from all fronts you know um increasing the memory we were talking about KV cache right. So KV cache NV uh FP4 and FP4. Earlier it used to be simply FP4 now it is NVFP4. Um that does the dynamic allocation of the the different numerical formats in order to take care of this the the latency part. >> Yeah. Hi my name is Adita. I work for a company called A2 solutions. We are basically into we basically into PLC's Alan Bradley's and Seammen's PLC's just wanted to understand from a data center perspective that was what is the cost allocation of different aspects that you have in your data center and what kind of investments are we talking about here in India that we are looking at to set up a data center here. >> So um the the yard stick is about 30% of the total infrastructure. The yard stick is typically 30% of the total infrastructure goes into the data center buildout. Right? So what it means is that the shell of the data center the power delivery that is coming into that and as well um when you're talking about uh the the CDUs CDMs and things like that as well as uh the UPS that goes into that depending on all those things that is basically uh around 30% of it but it changing rather quickly because it all depends on what you put in the data center. So for example in traditional data center that's what it is. If you go into the AI data centers instead of let's say a $4.5 million product suddenly if it becomes $9 million then the equation actually drops on that front. So um when when people talk about a gigawatt type of data center right um the power is one of the let's say on the operational expense side of it normally it's what 7 rupees per kilowatt hour for many of the data centers here in India. Now you need to start looking at whether you are using or not are you paying for it because it depends on the source of the power right but the data center is concerned again where you are building and you know whether any local subsidies that are available whole bunch of stuff right I I can't hear you sorry so Romesh or who is the one that's carrying the microphone can you give it >> can you just take an example and share like you know what exactly Exactly are the typical typical costs like not only of the data center that you have you have the cooling systems that you have there right you have a lot of heat exchange systems there input power generation systems that you have are approximate cost and you know split of that cost so that you know we are able to understand if we're getting some investments here what exactly are we looking at >> right so basically I I think that's something that we cannot directly give that it's cost function on that one we can actually take a specific use case we can take your contact information and work on let's say for example if you're talking about a 50 megawatt data center. What are the things needed? We can kind of spell it out on that on that front. That's easier way to do that. Thanks. >> Yes sir. Uh this is Omlan Shark. I'm from West Bengal State Electricity Distribution Company. We have a conventional data center. Let me tell you first and you have changed uh whole of my idea today. So uh we uh as we are talking about 250 kilowatt rack my whole data center uh is having a capacity of two 250 kilowatt. Now uh uh moving I have I have uh seen the u PPT that you have given give me an idea as you have uh talked about 250 kilowatt per rack. So uh what kind of energy you are using? What is your backup power? Say are you using green power or what is your backup power source? Is it running in DG generator or uh any secondary source? What is that? So couple of things. Number one 250 kow is not bad my friend. I mean it it it was the thing that was actually all of us grew up on right in the last two years. It kind of makes us look oh my god what happened? The point is there is still a significant amount of u standard compute CPU based compute that's happening where um majority of the workloads can still run. Um so that's number one. Number two when it comes to the power it ultimately boils down to the data center operator where the power is coming from. So let's say for example in your data center you want to retrofit and you say like you know what I want to use all my compute and storage as they're coming in but I would like to add some of these advanced GPU clusters now what happens when you do that you need power and you need cooling you need to do figure out a way to do that now you need to start looking at where is it coming from right so if you start taking a look at let's say for example West Bengal if there's some hydroelectric plants you take it and the grid is there and you bring it and you put a substation and you get the thing done fant Fantastic. But what if you need something in the next two months and this thing takes like a year and a half in getting the approvals and you know compliance uh clearances and everything then you will not be able to use it. Part of the reason why is because like then they say okay you know I can bring some turbines I can actually have a diesel generators to basically bridge the gap until my standard compute will standard power delivery will come into picture. So it ultimately boils down to what is available and within that you know what you choose. My hope is that you know there is enough renewable energy that is going to come in India whether through solar or through you know um wind or any of it although it's not sustainable right I mean solar only in the daytime wind you know I don't know when it comes um but the idea is to use however much that we can from the renewable energy which is going to be good because the amount of power that will be needed in the next few years is going to be insane to say the least right >> but presently how you're operating you are operating in >> I don't operate at a data center I actually work with data centers uh or a customer such as you and say like okay these are the potential options that you could consider in order to bring something into a data center right our main core competency is actually making the compute or storage or networking that goes within a data center >> okay but do you have any idea what kind of power they're using right now or whether they >> in India or where because we have >> in outside also I mean whether they have any battery backup uh and how long uh that is sustainable. >> Yeah, battery backups are basically used as a transient. In other words, let's say for example in a large language model training when you are doing it there are a lot of spikes. So in order to handle the spikes and not to load the grid is where the battery backups are there. Like for example uh Tesla has a mega pack that typically provides that people put outside the data center. It takes care of the spikes. uh otherwise what happens your generator will get wrong signal you know should I make it or not and by the time generator either picks up the signal and starts making other spike may come and you know do things in a different way um to answer your question it's a combination of various things there's no one or zero answer for that >> it's a mix of carbon energy as well >> absolutely thank you >> thanks hi uh thank you that was a very meaningful uh presentation uh the question that I was trying to ask is that uh from an overall net zero implication point of view. Is there a growing sensitivity uh overall in a at a national level that this needs to come in as one of the primary sources given the appetite for uh AI models and the way that we are consuming it uh because you know each of us know the amount of tokens we are passing on etc etc uh without being oblivious of the amount of energy consumption that is actually happening. So how is it that yall are seeing this sensitivity towards this cause actually rising across the organizations and is this moving towards something more measurable and tangible as we look forward? >> Uh very very close and dear to my heart type of topic because um we always are looking at energy efficiency is one of the key ways to reduce the power consumption. So think of like within a given power profile what's the maximum throughput that we can get. That's also a way to look at it is in a typical data center that's running at like 1.6 6 1.5 as a PUE or the power uses efficiency 50% of the energy is being burnt just in you know kind of cooling the systems right by improving the efficiency and making it run at a higher ambient temperature or using alternates like liquid cooling and whatnot we are able to in a way slow down the carbon footprint in a way many companies actually have anywhere from 2030 to 2050 time frame for you know carbon neutral or a net zero but the AI is coming in completely a tangential way in a very big way. So which is putting definitely a lot of strain on these companies. But what is happening is many companies because of their own initiatives they say I need to take this much of power from renewable energy. In fact that is also one of the reasons one of the reasons where people are moving into different countries where the renewable energy is readily available such as Nordics for example where you know environmental conditions are a lot more uh you know conducive to that. In India, what I see is that we have enough sun and then you know we also have enough water because of the dams and everything. People can actually use different ways of doing. In fact uh I was talking to some of the energy companies um where they are using uh dams to basically pump the water like think of like a closed loop uh where you have a big pond u you know below the uh dam and then you pump that water up into that thing. So it becomes a static energy and then in the night time when it's needed when there's no sun you actually take that and then go create the electric city. So the answer is that there's going to be huge investment that would be needed and I think companies will be forced to because of the government regulations also because the Indian government is also pushing for renewable energies and making things more efficient as a part of it as the scale increases then the cost becomes more reasonable. So private entities can still run a profitable business by adopting um you know green energy. So I'm sincerely hoping that it's going to further accelerate and get adopted. Thank you. >> Hi, my name is Suga Chit Gopkar. I'm a patent lawyer from Chicago. We do lot of patenting in this area. My question is have you considered placing the data centers in colder region to take advantage of the cold air and circulate the ambient air for the cooling purposes. It's happening actually you know like most of the times what ends up happening is so um let's say for example I go and put it in I don't know they're going to kill me but let's say Antarctica if I ever put it it's very uh because Arctic is melting anyway so if I put that it's cold but the point is how am I going to pull the data out I need to basically have cabling all the way there and do that right so there are different regions in the world where it's nice and cool and you know it actually without impacting the local environment much we can run it but um there's additional infrastructure spend that is needed which will eventually get there but what is happening is um Nordics as I mentioned is one of the growth areas um I think two three years ago a lot of people were talking about Iceland and I think it's kind of getting capped out now they're going into other regions um but unfortunately we want to bring the business to India and I don't want to go and put it in u you know Himalayas either right so the idea again here is to figure out with what we have we have enough sun we can create electricity with that and figure out a way to do that. And if you're doing in coastal regions where you can potentially, you know, use the water from the ocean and pump it and do things like that. Um, I think there's going to be a lot more innovation. I think the one good thing that we have plenty is a lot of smart people. I think we can actually figure out ways to do that. >> Hi, my name is Adesh Tiagi. I'm from Bay Area where super micro is from. Uh, we're looking to establish large data centers 500 megawatt plus in India. Uh, that's why I came here. uh how much of the capacity you have installed so far in India and where >> so if you take a look at u entire India the public information that's available is like you know less than 40 45,000 GPUs that are deployed so much of it is going to be in future so what happened so far is only like in tens of megawatts right that's the reality of it and the second thing is that offtaking is typically going to be on inferencing and where what I see is that multiple enterprises and whatnot are going to be using them with a smaller footprint. What I do see such as yourself first of all thank you for doing something in India uh with a larger deployments. I have seen um several initiatives running from few hundred megawatts to a gigawatt plus that's happening over the next two to three years. Uh assuming that there is an offtake and makes a business sense and whatnot. So what we have done is a part of those 40 50,000 GPUs in India. It's not a huge number compared to 100,000 GPUs or 200,000 GPUs that getting deployed in some of the other parts of the world, right? And that's why I think the whole AI um you know the summit that's happening right now is very important because there is a gap. We have lot of talent but we don't have uh the infrastructure we don't have um an offtakers that is coming from India. So when you take a look at the subsidies that is being provided by the government recently I think the tax break until 20 47 something like that some 20 year tax breaks and reasonable amount of power lot of talent all these things is hopefully going to um bring the infrastructure players to come here and start deploying things so it can be a very very uh powerful player uh in the world stage otherwise what happens the technical the the the services type of arm that super the whole India was very popular. If some of that is taken by AI then the economy what is that we are going to do. So I think it's very well positioned in terms of technology and the power and then the stability of the governments and everything. So we just need to see you know what we can do on that. Any other questions? >> Yeah. Hi uh I is Ish Fargo from electronics and semicondu sector. First of all it says a great session amazing you know information you have provided here in a very nice fashion. Uh my question was that can we extrapolate it uh or if you already have the full knowledge systems like cerebras like wafer scale engines how much they are able to uh achieve in terms of paflops per rack and how much power they so any insight or you know you can >> I mean it's easy to do that luckily all the chat GP and anything you can just drop a question and they will be able to you know do a comparison but what's happening is the larger the models that we are and uh the cost per token that is going to be driving uh the ecosystem. So uh if I were to take a step back, you know, Nvidia of course is a def factor standard. Uh we have like what 90% 95% of the products based on that. We are working with the likes of AMD and we are also working with um uh inference partners such as Furiosa AI um and rebellions and things like that. um all of these things why I'm mentioning that is um you take a look at uh many of the models that are primarily driven driven by the CUDA as an underlying infrastructure on top of it whatever the layers that you put on that it's working that comfort is what mainly driving you know people for something but then for those who are actually having the technical resources to go and run these infrastructure and develop some applications on it cerebras and all these are fantastic solutions so I have seen um you know for example them doing in Middle East uh as well as in some of these uh national labs and other things in in US uh quite a bit. Um it it ultimately boils down to you know what works out best and whether people have resources at one way people are saying I want to deploy it like yesterday. So then you need to look at you know what works for you versus in the other aspect of it is like well and I have a reasonable cluster and everything is running now I have to go back and see how do I optimize my cost function then this alternatives is going to come in so uh I think it's going to be in in the effort to things getting democratized over a period of time you'll have more players coming up with different uh uh differentiated solutions for an applications and that's what is going to drive it that's my take >> thanks one more you want to Uh one more question follow up to the gentleman's. So this 40 50,000 GPU deployment has been done by super micro is that >> that's what I have seen deployed in India. Part of it is by us. >> Oh great great that's very good. Thanks >> thanks. Anything else? So we have Cambis. I haven't introduced him at all. I mean he's a vice president of sales and um uh you know we brought him in from San Jose. So in case if you have any questions related to what's happening in India with respect to the business and what potentially you need something he'll be more than happy to address it as well. >> All right. Hello everyone. My name is Kembies. Um as Vic mentioned I am out of the headquarters in San Jose, California. So very happy to be here. Um it's a great event. Thank you to all the presenters we've had. Um we look forward for all of you to come to our booth and see all the different solutions that we were talking about today. uh we have our full team uh there as well who's covering the Indian market. So please come by, stop by, say hello and we'd like to meet all of you. All right, thank you for all your time today and you know we'll see you in our booth. Thank you. >> Any more questions? Uh hello I'm Kik Sharma. Uh I wanted to ask like uh with HBMs uh they are going to a stacked architecture. So is the uh is it also cooled by the liquid cooling method you have mentioned on the slide or uh that needs to be addressed with the air cooling only. So if you take a look at the GPUs, let's say for example um uh you take like Nvidia B300. The Nvidia B300 is a 1100 watt per GPU, right? Uh it has the GPU also it has its own HBM3 memories. It is offered both as air cooled or liquid cooled. So basically what it means is in an air cooled environment if you're running those systems that we showed like a 8U or 10U systems it's using air it's being cooled and a liquid cooled environment the cold plate actually covers that as well which takes care of cooling that using liquid right so both of them but as things progress and more layers are going to be formed obviously you need to take the heat out more uh you know efficiently in which case it'll eventually get into a point where it will be liquid cooled maybe Alo And >> yeah so good question so in fact with our next generation it is liquid cooled the next generation the system I was showing so that is liquid cool the memory is also liquid cooled >> yeah bottom line is it can be air or liquid that's number one to understand but as more layers like today um it's like first stacks were like eight stacks then it became 12 as it going to 16 and all now you need to figure out a way to efficiently cool it in which case eventually it'll go to to a point where it will be liquid cooled altogether. Anything else? Anybody still awake? So thank you very much. Um you know please I mean we are looking to see a lot more coming from India especially with respect to AI. A lot and lot of talent is here and then you know anything that can actually be done uh here that's going to be helpful for all of us and thanks for giving us an opportunity and uh sitting through this presentation. Uh hopefully you kind of got a better understanding of what Super Micro is and what we are headed and there's going to be continuous changes. You know if it were to be a follow on for this event next year uh maybe it will be completely different story. So we need to continue to stay relevant, stay stay active and aggressive and we'll all be good. Thank you very much. Bye. Mr. Raj. Hello. Hello.