Oh, something just like She's not Start to Okay, great. we'll uh we'll get started and uh then hopefully people will will make their way in. Um but first of all, I just wanted to say a huge uh thank you to the organizers of the event. I know it's been hectic with the traffic and all of the different venues. Um but it's been a real pleasure to work with all of you and you've been in constant communication with us. So, thank you. Thank you so much. Um, and a huge thank you um, to all of you for agreeing to speak with me here up on stage. Um, I'll have you uh, go through and introduce yourselves um, so that it is accurate. But, um, I'll just start by saying I'm Ashley Caspen. I am the managing director of the AI governance center at AP. We are a professional association that's really been thinking about how to develop um and foster an environment for people that are in digital governance professions um to be able to really have the community and the skills that they need in order to do the work that um they need to do well which includes um making sure that there's appropriate privacy compliance. um now the AI governance work and cyber security. And so we're really um I think just at the beginning of thinking what does it mean for AI to be in society in all of the different lines of business that uh we all work in. And I'm really excited today to try and narrow that aperture a little bit and focus on the implications of Agentic AI and what that will have in terms of how we need to be thinking about and developing standards. Um so I'm going to start with you Rachel please um share your background. >> Um hi everyone good afternoon. Um I'm Rachel Adams. I have kind of two hats. I lead the global center on AI governance which is a South Africanbased think tank that supports governments across Africa and all over the world uh to develop governance strategies that advance kind of equitable outcomes with artificial intelligence. We have a project called the global index on responsible AI which we engaged with Ashley about a number of years ago when we were first designing it and I'll I'll talk a bit more to that. Um and then I hold a research professorship at the University of Cambridge where I lead the Lever Hume Center for the Future of Intelligence. So my work is really around um AI and global inequality. That's kind of my big focus area and I I published a book called The New Empire of AI, the future of global inequality. So I'm very very heartened to be at a global event like this where those issues are front and center. I think we've waited a long time for these issues to be squarely on the table as the most significant promise and risk associated with AI. So looking forward to the conversation. >> John, over to you. >> Hi folks, thanks for being here especially so late at night or late in the afternoon I guess. Uh I'm John Dickerson. I'm CEO of Mozilla AI. So for those who aren't familiar with Mozilla, Mozilla is a venerable nonprofit that's been around for for quite a long time and um sort of had a claim to fame in the early 2000s uh for maintaining the open internet. So there was a lot of centralization of power in browsers around 2000 and Mozilla put together Firefox and Firefox uh was meant to and still continues to build community around full openness on the internet and more recently we've started to mozilify some of our other concerns. So, for example, something that keeps us up at night at Mozilla, something that keeps me up at night at Mozilla. Internet as we move toward an agent or AI based uh browser, right? Like if you have all of your information going through a closed source, then that closed source can start to control your access to information, your access to society, your access to social things, your access to the ability to book things on the internet, all the things that you've done on the internet for decades, that can now be and is at thread again. And so we're excited to be here uh in India. We're hosting a number of community events to start bringing that sort of open source AI community and in a basis sense decentralization, privacy, ownership of data, all of the things that we need to maintain the open internet for the coming decades. Um so excited to be here, excited to have this chat. >> Amazing. Thank you, Emanuel. Over to you. >> One, two, one, two. Okay. Hello. I am Emanuel Kway. I am the managing director and CEO of the UK division of VDE. Uh if you've never heard of VDE, the best way to summarize it is we're a nonprofit. We're about 130 years old headquartered out of Germany and primarily responsible for I wouldn't say understatement to say electrifying Europe. So we were there at the at the dawn of the electrical age helping industry adopt it both in terms of research and everything from research to technical guidance which we call standards which is how which is what we're known for a lot now within Europe and specifically within Germany where we house the electro the electrotechnical commission for Germany. Beyond that, we have spanned out over the last century or so to cover everything from electricity to the digital age and relevant AI is that uh we are responsible for setting up the European AI standards committee at Jets 21 where I'm one of the project leads um and also spinning up a lot of work with regard to AI governance at the OECD and other international bodies. >> Thank you so much. So, one of the reasons I really wanted to have this conversation with each of these individuals is because, as you can tell from their backgrounds, they're really representing different aspects of standards and I feel like um we're let's say five, six years into AI governance conversations. Um this is the third major AI summit like this and I think there's been the acceptance that yes governance of these systems that we want to adopt so rapidly is important but how do we go about doing that is been a conversation I don't think we've dug into in a nuanced way. Um, and a lot of what I've heard, at least in the past, let's say six months, and John and I were talking about this just in September, is people saying, "Yeah, standards, standards will solve everything." Um, but as you can even tell from the introductions, standards aren't standard. And I think that by, as I was saying in the introduction, putting it through a lens or an aperture of just a Gentic AI, even though that is massive, we can start to get into some more subtlety around what does that actually mean? Are we trying to compare things on an indicy? Are we providing or developing technical standards um or policy based that are adherent to compliance functions like the AI act um that Emanuel's been involved with? And I just want to add another layer to that, what we do within IEP is really think about then the people standards. And so from my perspective, one of the things to just get this started and Rachel, I'll start with you um to then talk about what does this mean in your context and how are you thinking about agentic AI as it relates to comparisons uh indices is that from um a professional association perspective, we're recognizing that we have to first define what skills do we even need um in order for people to work and implement and deploy AI agents um particularly so just with that sentiment would love to to turn to you about how you're thinking about this in your research. >> Yeah. Well, turn I'll turn the mic on. Um I think it's a really complicated question when it comes to how do we bring governance into this technology to kind of feed through not just mitigating the risks but feeding through the kind of outcomes that we want to see and we've seen so many I wonder if the word is difficulties in trying to adapt existing governance regime regimes and mechanisms to generative AI because it is a technology that evolves as it is deployed and I think with those old governance tools we're trying to figure out how do we develop assurance mechanisms and and of course standards that help us address that particular issue and you add agent AI onto that where you have not just your governing predictions and your governing outputs but you're governing actions as well it becomes even more complicated. So I think from our perspective I'm I'm largely ba well I I used to be based very much so in in South Africa and one of the concerns we always had about how do we develop standards is that standards tends or standard processes tend to be dominated by the places where there are the most resources and time to influence what these standards come to be. And then very often they can look like standards that are the kind of gold standard rather than the minimum standard then which can act as a kind of market filter or can be very difficult for countries around the world and particularly uh startups and other kind of smaller less resourced organizations to to meet. Um the global index on responsible AI is a kind of standard setting in some ways. I mean we we have technical standards and then we have standards that are much more like setting benchmarks for how we break down a concept like responsible AI into something that's actionable for for government. So it's a kind of assurance tool for the public or for multilaterals to kind of see how far a government's actually doing what they say they're doing when they're putting in place governance regimes for responsible AI. So what we have seen is that the index is a very valuable kind of tool to drive uh policy action through um peer comparison rather than just having a set of kind of political theater that that talks to um responsible AI but is is far from meeting that on the ground. We have two well we the first edition of the index came out in 2024 and we were very deliberate not to include generative AI or a specific set of indicators around generative AI at that point um because we were concerned that we have not yet developed a best practice for how we govern generative AI and it would be foolish to expect governments around the world who have already taken steps to govern this technology when we don't know the best way of doing it. And in some ways, this might have been the experience we have in the European Union of setting a kind of best practice governance tool that's now not been fully implemented because perhaps they, you know, went too far too quickly. So same with this new edition that's coming out now in 2026 in July. We spent a lot of time developing a new framework which sets out you know particular benchmarks for what responsible AI should look like and we haven't included anything specific around aentic AI but there are kind of tools like audits and other kind of assurance mechanisms that we are collecting data on. to watch this space for where we come. >> And I think that's really interesting. Emanuel, I'm going to come to you and then we'll go to John because I think just thinking about this in terms of standards from a more highlevel comparison that Rachel's talking about. Um, for those of you that haven't seen the index, I would highly encourage you to have a look at it. Um, but they are really highlevel goals. And I think it's interesting that you're mentioning even kind of meeting a minimum viable benchmark in a way that might actually be difficult for some organizations to to meet, some governments to meet. Whereas I think to jump to John right away would be trying to get kind of a best practice for companies that are um are really trying to look at a more nuanced version of what a technical standard is. So Emanuel maybe just to to go to you. Can you share a little bit about how VTE is and your work with Senlech are um approaching the creation of standards and some of the uh both opportunities and barriers there? >> Okay. Um so this a loaded question because it's [laughter] okay. How will I take this? Um okay. Okay, let's first start off by setting the tone of what when I'm talking about standards what I mean. So standards are all about capturing consensus you know um and there's other mechanis mechanisms c capturing consensus you know uh legislation captures consensus you know uh um some of the let's say high level governance mechanisms that exist in companies c capture consensus in some way but the thing that makes standards very different from for example legislation is that this consensus has to be captured and then there's an extra step of operationalizing it. It has to be delivered in a practically useful way. Uh so it's not just saying that we want our systems to be fair. We want we want them to be transparent. We have to go in a path to towards what transparency actually is. What is fairness? It's so I would say that uh there is a question of what do we want and one method of achieving what we want and that's what standards also pro they provide a method of doing so. It's not the only method but it's a method of doing so. Now when it comes to coming to your qu to your to your question with regard to Europe um when we came to delivering the standards or starting work on the standards for the AI act we were delivered this uh document this directive with a whole set of of of let's say the consensus of Europe what they want from AI. We want our systems to be fair. We want them to be transparent. We want them to be accountable. But nobody told us exactly what fairness actually meant, you know, because depending on who you talk to, which sector you're talking to, which country you're in, fairness is a very different concept. And this is the and this fell upon us. And this was this was the discussions we're having internally. Like we felt like the politicians kind of did the easy job and then they threw the hard job over to the technical folk to be like okay then sit around and within two years try to make all of this practically useful. You know all our wishes and dreams practically useful. And that's essentially what's been going on in Jets 21 for the last two years. We've had uh let's see uh a coalition of experts spanning everything from law to um medical uh medical sector to even nuclear nuclear sector to even defense who are sitting around uh the table and coming to shared upon terminology for example because like I said we don't agree on what fairness is uh metrics you know um I among other things trying to come together with a shared notion of what can we agree on and standardize horizontally across the board and furthermore once we've done this horizontal standardization what is needed to consider the sectors and interestingly within Europe this has been the first time I would say that standards have been used as part of the machinery of delivering uh on a directive um before we had we had GDPR which was a law you know but there was no standards and there was a big mess there. [snorts] Um, and so this time round they were like we're going to get the standards book involved in the beginning. And so we've ended up with this kind of like let's say back and forth between what has been written in the law versus its interpretation in the standards. And what I can see is that this is the first time that the European European Commission is actively in parallel as we're developing the standards providing feedback. This is something that's never had happened before is in that as we're developing it, as we're developing the standards, we're sending them off for active review in parallel because we have a strict timeline. We have to deliver them by the end of the year. So I will say that that's the European approach. But then complimentary to that there this this set of standards are being developed to as a response to the AI act the legislation and they are different to the kind of standards that industry might want because you can create standards for compliance to help you comply to a law but you can also create standards for best practice to help you compete to put the best products out there and those two are not necessarily the same thing which confuses a lot of people and so there's also another crop of work on standards for industry and this is where you and end up with some of the work that we've done within VDE which we have a trustworthiness standard and it's a graded standard you know it's not yes or no but it's a graded ATG and it allows you to as an organization to be able across these principles of AI accountability transparency depending on what your market requires of a product depending on what your the let's say the societ the society within which you operate demands of a particular uh product say for example Germany cares a lot about privacy, then maybe as a company you can invest more in, you know, showing that your pro your product is more privacy preserving than another product that's out there. >> Yeah. And I think that kind of goes to this point that Rachel's making around just having something that is achievable but also achievable for the market. Um, and I think that this idea of having gradient standards, especially as we're starting to think about implication or uh implementation and increased adoption of AI in various different markets, but using the same platform technology is um something that's going to need to be created in a more nuanced way, which I think John leads me to to you and the work that I would guess say that you're doing, but also that you're thinking about in terms of guess these roots that Misilla has in terms of the open source community would love for you to speak to that. Um but then also one of the things that um Emanuel's saying but not saying is that so I'll say it um is that how that process of creating standards has been done through the AI act is slightly different but it's still st or it's still following a really formal process. um who's invited to the table for that consensus is not necessarily the same as would show up for an open source group or even the research group that came together to start the the responsible index. Um, and so I think that that's something, John, that I'd love you to speak about in terms of both um, what is being worked on and how that's contrasting to what Rachel and Emanuel have spoken about, but also um, who's involved and the different process that's being taken. Yeah, that's a great question and I I'll start by saying something that I've heard in in both of the responses so far is that there's a an opportunity for better communication both communication from regulators and lawmakers to standard setters and from standard setters to startups and investors. So one of the things that my last company was involved in was something called the responsible innovation labs uh which was founded by some big VCs in the US as well as a bunch of startups. And the pitch there was that we wanted to get together and try to form a communicative framework and potentially standards recommendations that would be friendly toward innovation but also be friendly toward government and regulatory sort of top down pressures. And it's a really tricky thing to do, but it was an interesting coalition that was put together to do this. And the reason that I was and still am interested in that sort of push is that, you know, we talk about trust in AI, but we also need to talk about trust between the competitive marketplace and standard setters and regulators because there's always going to be this adversarial tension between um this is happening in a I promise I'll answer your question. Um this is happening right now with Agentic AI where you see standards that are being set, best practices for competition uh by large companies, right? The Agentic AI framework that was recently booted up on the Linux Foundation has working groups looking at a lot of very very important things like identity uh commerce u uh robustness to you know adversarial attacks things like that and agentic AI. Um but it is entirely grassroots as far as I know coming up from uh the big frontier labs and then other folks who have contributed to both open and closed AI and that can take us in a direction that might be not you know where the the global optimum for uh the US in terms of agentic AI or the entire globe in terms of agentic AI is and the reason that that's happening from the bottom bottom up right now is because there is that lack of trust right now between the big tech companies and people who are going to set standards and people who are going to set regulations Um, and so, you know, to to your to to to your question, this is happening right now all over the place in Aentic AI. And um uh to Rachel's point as well, like not being too early, putting hard language about, for example, an emerging technology like a like engetic AI into the index right now, I think is the right move in the sense that we should instead be trying to elicit industry, society's preferences and values with relation to this evolving technology and then trying to maybe three or four years from now have that sort of trust between people who are pushing the envelope and trying to understand what can and can't be done with the technology and those who are going to be setting the inevitable standards that guide where we're going to go. And you know, I think it's just something to keep in mind in that there are like so many players in standard setting in general. Um, if there's no trust in those who are pushing the technology forward and those who are setting kind of the rules for it, then you're going to see folks pushing against them and breaking them and ignoring them. Um, which is just a dangerous place to be. And frankly, I think we've seen that in AI both before Gen AI as well. know I've done a lot of work in in regulated machine learning like uh in healthcare in banking and things like that and those have very in various countries very very strict I would say top down controls over what can and can't be done and I didn't see an open dialogue I would say as machine learning began to progress between those who would wish to put that into production and understand sort of where the new pain points might be and most of the regulatory environment Uh, for example, in the US, there was a little bit of work with our CFPB where they put together a sandbox where startups that were trying to do things could kind of tell regulators what they were going to be doing. And then, um, as long as they were very open with the regulators, you know, not be put in prison if something went wrong because they didn't know what could possibly go wrong. And I think those discussions also need to be happening uh, as this technology uh, moves along. I think we're going to see that again in finance as people move into a I think we're going to see it in healthcare, but we're going to see that across the board. Yeah. And we've even seen where Google and other companies have put together this AP2 standard for agentic ament agentic payments. And so I think that while in contrast, Emanuel, I'm so sorry you're getting like painted with this European brush. Emanuel in his work moves very fast and is trying to work through this. Um, but I think just for purposes of this conversation, we're seeing a very um formal process evolve for developing standards against the AI act um and then um requests for it to move faster, which is a whole separate conversation contrasted with then there being a need for agentic payment processing and therefore a standard being stood up quite quickly by industry. And so I think that um this is again at the heart of it really trying to understand I like how you put it earlier Emanuel understand what um those objectives are and then bring um the people together for the consensus. Rachel I'd love to turn to you though from a global south perspective um can you just share a little bit about then as these various different levels and types of standards are being developed where is the global south in these conversations? Do you feel like what's being developed for the AI act as part of um the ISO efforts or sensic efforts but then maybe harmonized by ISO that formal process? Um do you think that will work for the global south? What do you think? Um what are you seeing? >> Yeah, I think there's a a few layers to this question. So I think firstly the kind of western model of AI governance if I can kind of call it that expects robust wellunded regulatory bodies. Um, and that's what in the global south we we don't have to the same extent. And I must caveat this because I think there's a number of very kind of there's a there's a role that a number of independent state institutions in South Africa and in Kenya are doing that is really really outsized a kind of whole big tech account and and various other things. But by and large our regulators, our independent state institutions are really really overwhelmed. So I think we do need to be thinking about kind of standard setting as something that's slightly more a process of capacity building and kind of little bit more modular. So less onesizefits-all certification and thinking through more kind of sandboxed approaches where we can learn together to build something that's a little bit more viable. Then on the kind of other side of things what we are seeing is like the kind of evaluation data sets that are coming out just not relevant for many global south contexts. When we're talking about generative AI, we're talking about, you know, are its outputs effective and useful? Are they culturally appropriate? And then are they safe? And so the questions that you might be using to evaluate and test a model in one part of the world are just not at all relevant and useful for measuring those things in another part of the world. So we really want to be seeing kind of more regional um evaluation data sets and we want to we we're thinking about and somebody on the previous panel spoke about this but we're launching a kind of global south research network on evaluations and assurance where we can share methodologies best practices and even kind of data sets around evaluation. And then maybe the last point on what we need to see what what we need to see is um global south actors being funded to participate in standard setting processes. Uh for their contributions to be as co-authors not as just seat at the table. We want to be seeing them playing kind of leadership roles in in committees. Uh and then of course we want to be seeing kind of regionally attuned whether it is kind of evaluation data sets or kind of assurance models that are uh appropriate for different cultural contexts and languages. Certainly a lot of significant challenges and I think talking about the resources is such an important aspect of standards development. Um and again, especially when we're thinking about some of these larger consensus processes that are in person far away with um high level subject matter experts needing to take time off of your other work um to be able to participate in that is very different than um again maybe some of these more um niche but very narrow um industry developed standards. And so John would love to turn to you maybe just to get into then. We've been talking a lot about the um the different types of standards, the different types of processes, who they're impacting. Um but if we could just maybe switch to what do we need standards for in the agentic space? Um what are some of the biggest risks and challenges that you're seeing? >> Yeah, I think one of the biggest that's a great question in general and actually I wanted to give a quick just to the last conversation here. I wanted to give a quick shout out to uh to Rachel's point about we need evaluations that are contextual, that are geospacc, that are community specific. There's a really cool group booting up that roughly goes under the name evaluating evaluations which um has I think support from hugging face and a bunch of other folks sort of in the responsible AI fairness and machine learning community and I'd encourage folks to check that out. what I mean you can tell by the name but like they're very interested in kind of doing that meta evaluation of what evaluations look like from a nuance point of view from a group specific point of view and so on. So I think there are small groups that are starting to really push forward on like hey we shouldn't just be doing generic average case leaderboard chasing because that's not going to service everybody across the globe. But it's very early days there uh in much the same way that you know 10 years ago it was very early days in the translation of the responsible AI community's uh you know philosophical work into practical standard settings into communication with with government officials uh and so on. So you know I'm bullish there's a lot to be done here but I am I am I am bullish for sure. Um, so your question was about uh what what are the big issues in terms of like standards for for agents. Uh, I'm not going to say this is the biggest issue right now, but actually we brought up payments. I think payments are going to be like this needs to be settled very soon because there are new economic questions about how the internet is going to uh continue to coexist as automation becomes more and more as agents are making decisions about purchasing or about negotiations uh in lie of human imp uh uh um human oversight or you know joint with humans and um money is just going to drive that and is already driving that forward right you saw this I'm not a crypto person but you saw this with like Coinbase putting out a lot work in agentic commerce early on because it was much easier to pay with cryptocurrency or stable coins than with traditional fiat currency and standards are now going to keep chasing after that. You know, you mentioned Google, we're going to see a lot of others like that and so payment is very very important. I think we're going to crack that though and the one that keeps me up at night a bit more is, you know, the the safety and robustness stuff around um around Aentic AI and the sort of like new attack surfaces that can open up in that space. Um there's something that I fundamental fundamentally believe um and I'm totally blinking on the name of it but it's this idea that basically if an agentic system has three things that is fundamentally going to be breakable and this is not in standards yet but those three things are the ability to have untrusted input coming in access to private data that you wouldn't want to share with somebody else and the ability to egress that data to the public internet and so an example of this might be if you're using an aentic system that's able to read your email, I can send you a crafted email. That's untrusted input. Your email itself is private information you wouldn't want to share with me. And then if I allow you to then email somebody else based on that this very basic agent. You know, people probably use these all the time. Uh then you have now have this um lethal trifecta, I think it's what it's called, um where you're able to break the system. And I fundamentally think we need to start seeing standards that look at the sequence of actions that agents and agentic systems take and put sort of risk scores or this is fundamentally broken scores on those little subsets of interactions. And that will allow regulated industries but also consumer applications to better communicate to end users, hey here's an attack surface, here's what might happen or here's what might not happen. And I haven't seen standards in that place yet. But this is something that like as we develop agentic AI and as this stuff goes into deployment both in like a YOLO mode from like a consumer point of view, but also from an enterprise point of view um we we need to have something there that gives a risk score. >> Did you say a yolo mode? >> I sure did. >> I love that you only live once. Okay. Um, yeah, I think actually that's a really great way to summarize how a lot of this implementation and drive towards adoption is happening right now is there's if we don't do it now, we're going to miss out on adopting. And whether you're thinking about that from a company perspective or a country perspective, everyone is living in this fear. Uh, and I think that taking that step back to ensure that there's appropriate guard rails around this so that we aren't sending private information out of our email by accident. Um, is interesting. Now I can just summarize that as yolo mode. I love it. Um, Emanuel. Okay, so John's just outlined and articulated some of the challenges. Feel free to add your own into that. um you sent me an article 12 seconds before this panel started about NIST creating a AI agent group or for standards development um NIS being the standards development agency in the US um are you starting to see more of these teams um be developed in other nations? Is this something that's being talked about in Europe? Um and I guess what risks are they working to evolve >> or to address? Sorry. >> Okay. Um, all right. Uh, I'll take that in two parts. I'll first respond to something that he said just to because Okay. So, when it comes to standards, first thing about standards, at least traditional physical world standards because I want to make a distinction between the way that folk who deal with safety in the physical world see standards versus the digital world. Standards that you expect out of a digital product, you would never use it, you know. So there is this distinction between the physical world and the and the digital world. Um the second part I want to raise with regard to the whole discussion around standards for agentic systems is that um standards are not built in a silo. Most standards reference existing work. When we talk about uh for example quality management, we talk about risk assessment there's already if you look par sector there's already not only standards but already existing regulation. And for the most part what we've and this is what we've also done with uh with AI is that we just horizon scan the best practice that currently exists and then we see what is missing for this piece of technology that what is new about AI that's not covered and now this ends up within Europe and now I'm going to go into the into a little bit of the legislative space um within Europe it ends up within and when I say Europe I'm including the UK it ends up with a a divergence on views. So if you go to the UK, the UK sits there and says we already have a set of uh let's say laws and standards that dictate our sector. We have sector specific considerations. We have sector bodies and all they have to do is evaluate reevaluate what they have already have in place and see is it suitable to cover then to cover what is uh being brought forth by generative AI and AET AI. And I've sat down with the FCA and I've had these discussions and they're like, well, we we measure risk and harm from an outcomes perspective. So we don't care about whether you're using generative AI. We don't care if you're using an if statement. We don't care if it's a generative, but we are going to evaluate you on the outcome. So you know there is the and so that means that the standard when it comes to standards development within somewhere like the UK the standards are mainly for industry to work better together rather than to deal with the risks posed by agentic systems because those are according to the governing bodies of of the different sectors those are already considered under existing law. So this is an important thing already covered over existing law. Now within Europe again um there was an specific AI directive because they thought that AI is significant significant enough as a technology to want to warrant this but as in the standards development process a lot of the work that we've done when you look at risk assessment quality we've literally seen what's the best practice that exists and what needs to be added on to meet the compliance requirements for to the AI act and that's uh that's to kind of like almost say Yes, there is worry that we do need to create standards and uh but the question is if you're talking about standards with regard to harm and safety of people, there's already if you talk to a lot of the regulators, they say that we already regulate for this and we don't care about the technology. But then when you're talking about like I don't know payments, people making payments uh or agents making payments, this sounds to me like standards for industry to compete or collaborate better together rather than to deal with harm because harm is already covered under existing uh legislation and regulation. Um and now to go on to the question about the the NIST and international collaboration. Of course when we started this work on especially within VDE we preceded the work at uh sensor like 21 on trustworthiness and at the time when we looked around we did a meta analysis of everything everyone was saying about air trustworthiness and we found that and this is how we built our standards actually our trustworthiness standards we just found that everyone's talking about the same things >> you know at the end of the day everyone wants cares about fairness they care about transparency they care about like we've not formed a single like we have consensus that we actually care about these But the precision of what we of what we mean by fairness changes depending on the jurisdiction you're in. That's where the differences come in. It's not uh it's it's not that we don't care about it. It's like how much do we care about them in what particular context. Um after learning this what we have done is we've taken a lot of time to try and not only incorporate what we've seen globally but also start to collaborate you know with Asia. we got AI verified that we are in contact with you know with NIST prior to Trump we actually were looking at uh you know a strong collaboration and then everything kind of went out the door for a while and then today I say that NIST seems to be getting back on its fit feet with regard to AI standards and regulation um AI standards and so we're hoping that once this happens that because the digital space is interconnected we're going to be looking at points of alignment you know how do we align because we might be asking we're most likely going to be asking for the same things the question is How do we translate what how you've worded your requirements to the way we word ours? >> Yeah. And I think for those of you that haven't seen it, um, NIS does have a profile for generative AI. Um, so it'll be interesting to see if they're coming out with a profile for Agentic AI, which does document all of the different types of risks that could come from generative AI systems. And I think that even just having that consensus as you're putting it in terms of what are the harms that are coming out of these systems is really useful. But I think Rachel to go back to that concept into some of what you've been talking about and thinking about this in the global south context. Um all of you have talked about making sure that the inputs coming into these standards developments are from people that are going to be using and working with these systems. um but often in the work that when we were talking about the start of the responsible AI index talking about making sure that people were at the table that are also impacted by these systems and so that's not always the case for um traditional standards development processes. So would love to just hear from you what meaningful inclusion from people in the global south would look like in terms of um conventional nonconventional standards development. I think that's an amazing question. Um, yeah, because I think even if you had a sort of collection of tech bros from the global south going and participating in a set of standard setting processes, it's not necessarily going to represent those that are most impacted. Um, and I think this is as well a really complicated question. We just did a public perception survey in South Africa, very very comprehensive one with the Human Sciences Research Council that's been running public perception survey, social attitude surveys in the country for over 20 years. And so we asked over 3,000 South Africans from all walks of life in all of their language groups. And there's over I think there's like 15 official languages in South Africa. We translated all the questions and there were questions about you know um what do you understand AI to mean? Have you heard about AI? Where do you get your information about AI from? What do you think AI should be used for? Do you feel excited about a future with AI? What are you worried about? The kind of headline was that twothirds of South Africans have never heard of well have do not know what AI is. So onethird of South Africans have never heard of AI. another third of South Africans have heard of it but couldn't begin to say what it was. So the vast majority of people and and I think we need to kind of unpack what that means. It's not just that they don't know what AI is and you know they've heard this thing and and they know it's really kind of important. It's actually not as important to the majority of people as I think we all think it is. it's it's not kind of immediately relevant for a lot of people and and we have to think about well where does the responsibility lie to build literacy and awareness where we know full well that South Africa and many other governments around the world are here saying we're going to be using and adopting AI in the public service we're investing huge amounts of money in uh building public infrastructure and I think in the global south we may well see agentic AI coming into people's lives through public infrastructure and public delivery of services rather than through consumer markets which is a very different thing and there's a different set of responsibilities. So then I think we need to be thinking about well what does kind of participatory governance um look like within spaces where there's low literacy and awareness around what AI is and and that's why I think it's kind of quite heartening that a lot of global a lot of AI strategies in the global south are talking about building public awareness are building about building public literacy and are really kind of trying to think through how do we build a citizenry who is thinking critically about AI because the other thing the survey showed is that a lot of South Africans think AI is going to change the world and be great and if you look at public perception surveys of AI in places like California and Silicon Valley where there's a huge awareness everyone knows what AI is the more people tend to know about AI the more skeptical they are so it's a it's a it's a funny balancing act. I feel like I mention this survey nine times a day, so I apologize um for some of you that have heard this already today. Um but there's a really great survey that I love out of uh the University of Queensland and KPMG that's looking at global trust. And it's funny because my interpretation of some of those similar findings were that the lack of trust in some of the adoption in places like California, just to use your analogy, uh were lower because there's other alternatives. We have other mechanisms um in the west to achieve some of the services that are being provided. So I actually think that there's a lot to unpack within these trust surveys and I think it's a good would love to talk to you more about this. We're running out of time. I know. I promise the organizers will stick to time. Um, but I think that um, John, you've been talking about Misilla being this home for openness and and working on open internet issues for so long. When you're thinking about development of AI in an open context, how do you balance that openness and trust >> for the standards or in general openness? for sorry I meant for creating standards if you can speak to it in terms of the open source technology. Yeah, I think you know when it comes to very very technical bottomup standard setting openness I think is the only way to go because you to the point that we've we've all been making here you know who has a seat at the table for some of these standards that are about let's talk about like agentic payments or something like that um having the traditional sort of open source community-led technology exploration quick prototyping and then standard setting that led to the web as it currently is right like Mozilla also does obviously see a lot of maintenance of web standards. Um, you know, as as an owner of a giant browser. Um, I think that's very very unimportant and I think if that weren't open and if the community weren't there then you would have a completely different set of voices at the table and I think they would probably be the kind of voices that lead to the kind of regulatory capture that is not a good thing. It's going to be the loud incumbent players who are generally closed and trying to do kind of the bad thing where they want monopoly power and so on and so on which has happened you know across the world with finance and with payments uh for for quite a long time uh is one and then for two I mean I think you're asking a more general question around how you balance open source technology and trust and you know this isn't new necessarily for AI we've had encryption discussions uh you know you have openness around anything that can be can be used for bad as well and I think it's just having these dialogues having these large global meetings and then having the ability to uh bring voices to the table that are extremely technical that are downstream not caring about the tech but are going to be impacted by it that are deeply embedded with standard setting industries with regulatory industries and so on. Um having all those at the table in the same way that they have been for decades now in tech uh I think is the way that we have to do it here. I just think that right now there's a lot of hype and I would say opakeness, purposeful opakeness around AI and what it can and can't do. And a lot of that is being driven by tech and by investors and so kind of pulling back some of that veil and making this more approachable to, you know, the rest of the world that aren't the tech protype, I think is is something that we need to work on on the AI side of things. >> Yeah, I really appreciate that. And then just in the last couple of minutes that we have, would love to hear from each of you. I think to go back to where we started with all of this um how can people get engaged and involved in um these discussions and what's one recommendation you would give for everybody here in the room that's uh maybe not yet ingratiated strongly into the standards community? Rachel, I'll start with you. >> Uh start a Tik Tok on AI standards >> standards. Cool. >> Sound is cool. Talk about it with a wider group of people. Um, spread the news outside of rooms like these. >> Your dinner table, everything. I like it. Marketing campaign. Okay, great. I'll give maybe a practical tactical thing to do and that's you know many of us in this room will identify in a siloed way and you know we're all part of multiple silos but you might identify as a software developer first or you might identify as a product manager first or you might identify as a policy maker first or you might identify as a uh you know an ethics and society person first etc etc etc. I would say pick one or two others that identify outside of your sort of core silos and just practice talking to them about anything in the space that might concern you and try and try and understand that gap in language. And to Emanuel's point, you know, there is a huge gap in language across industries, across countries, across types of people. Just try and practice that and understand how to get how to get better at that. >> Wonderful. Emanuel. >> Well, I'll start by saying that standards are already cool. like that's a [laughter] so uh I mean uh I didn't think that when I first started but uh given it's one it's one of the few places where you have an op you have the ability to like I said the question of who's around the table I think that um if you are in pretty much every country has a national body if you want to engage in standards they have a national body for standards and if you represent a particularly unique view there is no reason you should be prevented from joining actually Because as again standards are built by consensus and literally it's supposed to capture societal consensus. So if you feel like your view is not represented you can actually apply to join your national committees and come to the international standards committees ISO IEC etc. And this is a way of almost contributing but all not only contributing but having your views ingrained in the way a technology will be deployed. And what I'll say is that that's pretty cool and you can do that. So you should go ahead and do it. >> So show up. Amazing. Well, thank you to all of you for joining us at the end of the day and thank you again to the organizers and uh to my incredible group of experts uh that have shared the stage with me today. Thank you. Thank you so much. Thank you.