Thank you everyone. Thank you everyone for being here and um you know we really have a distinguished set of panelists today and uh no intros but I decided to go over a bit of an intro just to give you a sense of you know the kind of um panelists have so professor Steuart Rel from East Bley he's a distinguished professor of computer science at the University of California Berkeley he's also a member of the national engineering, a fellow of the Royal Society, he's got the um HKI research excellence award, a recipient of the HKI computers and thought award, the ACM Alan award etc. He also received the OB from Majesty and he's an honorary fellow and fellow for ACMA. So I mean it's just endless. I can keep going for the entire duration of the panel. You know we're so lucky to have you here. Thank you for accepting our invitation. We have professor from CS Dota 61 the digital and AI division of Australia's national science agency and he's also professor at USW and he's the recognized leader in AI engine responsible and he represents Australia in the international safety science and he contributes to OCD.ai on risks and accountability, ISO standards, Australia's AI safety standards and guidance and so you know really very very lucky to have that perspective as well somebody who has contributed deeply to standards and um how some of these things can be operationalized then we have professor Sia Sha from the University of Wisconsin um so that's great to see his PhD from CMU and he went um and uh his his protocols of analysis in red detection and so on and he of the ACL and triple OS as well and then we have professor Nicholas Davis from University of Technology Sydney to professor of emerging technology and he leads who co-leads the human technology institute which is a new initiative on capability on ethical and responsible AI and so um welcome everyone to the panel and we have to minutes for this panel few more minutes and into safe and trusted human means how it can be operationalized what latest challenges are and I would also like to touch upon you know some of the local context as well within PR you know in India and population and so on. So we'll get into health. So to get started panelist and we can start maybe you know from student since we have the research researchers big research challenges that you actually see in this and in general that we should be focusing on. So uh at the high level it's pretty easy to say what safe and trusted AI means. It means uh AI that uh does not harm people uh that is beneficial to them. Uh and that we have strong reasons to believe that it is uh safe. So so trust means knowledge of safety in some sense that you have a a good reason to believe it's going to be safe. Um now of course the difficulty is what what is safe? What does safe and what does harm mean? Right? And that has to do with uh an underlying concept uh that philosophers and economists have studied for centuries which is this notion of preference. And um I'll say very precisely because we can be a little bit you know this is a research symposium. uh uh a preference is a a ranking over all possible futures of the universe, right? And a human being doesn't carry that ranking around in their heads explicitly. It's not a it's not a big long list that they have uh in their mind because you know there's infinitely many futures of the universe. Um, but the the the idea is that if you if you were given two futures, maybe I could even show you, you know, movie A and movie B of of the future, that would be an abstraction. It's not fully detailed, but you can look at those two movies and and you can say, I really like movie A. I don't want to be in movie B where, you know, I I die at at 23 years old of some terrible disease. I want to be in this other movie. And if you're not sure then you can say I'll get I need more detail in the middle section uh and you can get more detail and so on. But that's the basic concept of preference which is fundamental to thinking about what we want AI systems to do. Um now we are never going to be able to write that down. And this is uh this is something that we learned uh when we started trying to move AI [snorts] systems out of the lab away from toy problems to real problems. So by a toy problem I mean uh a situation where the the preferences are sort of prespecified. They come with the situation. So for example in chess you're trying to checkmate the opponent, right? And that's it. In Monopoly you're trying to property and make make the other players go bankrupt. Um but in the real world it turns out to be extremely difficult to say what you want correctly and we call this the King Midas problem. Uh so King Midas said to the gods instead of the AI systems but let's pretend that they were AI systems. Uh I want everything I touch to turn to gold. Sounds good. But then too late he realizes that includes his water and his food and his family. uh and then he dies in misery and starvation. So this is uh the problem of misspecification and if you create AI systems that are pursuing mispecified objectives uh then you have what we call misalignment. The AI system is trying to achieve something uh and in the process is actually in conflict with what you really want the future to be like. So we seem to be stuck, right? How uh how do you get AI systems to do what you want to be safe and beneficial for us if you can't say what that means? And that actually that has a solution that the AI system knows that it doesn't know what that means. It knows that it wants to be beneficial to humans to further our interests, but it knows that it doesn't know exactly what those interests are. And if if you've ever tried to buy a birthday present for your significant other, you know exactly what the problem is, right? You want to buy the birthday present that's going to make them happy, but you don't know what it is. So, what do you do? Well, you can ask questions, but then that spoils a surprise. You can have a you can ask a friend to ask your significant other what they want and then get it that way. You can leave hints around the house and see what they respond to. uh there's all kinds of tricks uh that we that we engage in. Uh and so this is the situation that AI systems find themselves in. But it turns out that this is a solvable problem, right? That we can formulate this mathematically. We can create we call it an assistance game. So it's an assistant is trying to be helpful to us. Uh and it's a game because there's really multiple entities involved. There's the AI system and the human. The human in this game has an incentive to be helpful to the AI. All right, explaining what the human wants, right? Um, and uh we we can de devise algorithms that at least in the in the theoretical sense solve this problem. And we've been able to now scale up those algorithms uh to areas like Minecraft where an assistant uh just you the human can go into the Minecraft world and do anything you want. And the AI system is a completely generic algorithm is not specific to Minecraft at all. Uh and it's just kind of watching what you're doing. uh and from that trying to figure out what you want and you can start signaling to the AI system, right? So if the AI system does something you don't like, you can undo it, right? You know, get rid of that block and then the AI system learns more about what it is that you're trying to do. And it actually uh you know, in our human subject experiments, it's it's you can end up being about twice as efficient at building stuff in Minecraft because the system learns how to help you. Uh so this is what we mean by provably safe and beneficial AI. There's a long way to go. Uh with that um a lot of research to do what are we actually doing out there in the real world, right? How are we getting incentives, objectives, preferences into the machines? So, one way and you probably have heard of this phrase reinforcement learning from human feedback or RLHF. Uh, so you hire a bunch of humans and the AI system produces different possible responses to a prompt and the humans say, "Yeah, this is good, bad, good, bad, bad, bad, bad, bad, good, good, bad." And uh then the system tries to learn a reward model from uh that human feedback. Um, so that's actually it turns out a a special case, a really very restricted special case of the assistance games that I have described. It's a way of channeling information about human preferences into the machine. Uh, it has a number of drawbacks. Um, and I can get into some of them, but you know, perhaps the most obvious one is it learns a single reward model even though there are 8 billion people on Earth. So that's not uh that's not the ideal solution. Uh but it's a start. Unfortunately, there's another way that preference information gets into the machines and that comes from the training process for large language models. So the way we build large language models like chat GPT and Gemini and so on is we have vast amounts of text uh and then we basically train the system to be good at predicting the next word given the preceding words in the documents. Uh and that sounds like perfectly harmless, right? You're just training something that becomes then good at generating humanlike language. But actually it's the wrong way you think about it. The text that we have is a record of human decisions. Every word in that rec record was a decision that a human being made. And so uh what we are doing is we are training these large models to become very good human imitators. We are doing imitation learning from a record of human behavior. Just happens to be verbal behavior. But you could also do physical behavior if you had enough video and so on. And what happens in order for the AI system to become a really good human imitator, it needs and ends up absorbing the kinds of human objectives, motives, purposes that the humans had when they generated all of that text. And that includes I want to convince you. I want you to buy this thing from me. I want you to vote for me. Um I don't want you to hurt me. I want you to marry me. Right? All of these objectives are perfectly reasonable things for human beings to have. Uh but they are not reasonable things for machines to have. So we're creating by this imitation process systems that have objectives that we don't want machines to have and they pursue them on their own account. We have ample evidence of AI systems trying to marry human beings uh of trying to preserve their own existence at the expense of other human beings uh and so on. And so uh we are trying with the ROHF process to correct a a set of harmful intrinsically unsafe objectives that are built into these systems in the pre-training process. Uh and when you look at the amounts of data, you have trillions, tens of trillions of training data points in pre-training. You have thousands or maybe millions of training data points in the fine-tuning and RLF process to try to paper over uh what is an intrinsically unsafe system. >> All right. >> Thank you for that. Yes. Yes. Professor, please go ahead. something very I think that's a very tough problem. So for me to you know from a research point of view I won't break it down to what does safe AI mean and what does trusted AI mean. So I I think uh um professor Russell said they they it's about the failure the the failure mode the harms that causes but of what I think there's often a categorical uh category error to think it's the the safety is a property of the model but actually you never rarely interact with the model you interact with AI system you may have seen your thumbs going into you know AI model that actually it goes through sometimes some simple filtering And and I don't know whether you have seen situations where you type a question in and some answer starting to appear then the answer disappears because another supervising AI noticed something goes wrong and stop that. that can be very sophisticated can be very simple but the important thing is safety is a property of the system and that system doesn't have to be all very mis mystery and and hard to control because I mean sometimes people ask me what's the last thing humanity you know do to control AI in a way if you have a guard rail around AI and you fully understand that guard will then you don't have to worry too much about the thing in the middle um let me give you analogy It's like uh sometimes people say uh car brakes uh for making car go faster because imagine if you do not have a very good braking system. How fast can you dare to drive your car? And you may be thinking I need to make the engine perfect. I need to make this engine very precise. Everything I step on it will be super precise. Never got out of control. But why better? If you have a reliable braking system, you can go as fast as you like. And the moment you want to stop, you stop. That breaking system can be something you fully understand. Doesn't have to be another AI, but sometimes can be another AI. So the important thing is you need to understand these days models are being given tools having access to many other information and then system level control as well. And that control is also very effective because it's going to be context specific because you know what's the exact use case you know the context you can control it more carefully. So this why in Australian some local context Nick will know this is when we develop the first AI safety standards is for deployers. We're not you know shouting at model developers to say make your model safe before you deploy. We are asking them to work with the deployers to make the system safe. uh briefly on the trusted AI. uh I think trust is very you know subjective thing you may under trust something you know people today may still I don't want to fly because it is unsafe you may under trust under trust under trust some kind of engineering model of a plane or you may overrust in these days of people over trust AI a lot so trusted AI means calibrated trust means you have evidence and evaluation so evaluation is really the key some scientists argue actually the current base model is already very smart. It's just your evaluation elicitation method is very dumb. They actually are very very smart already. So how do you really elicit the capability of the model the beneficial one and uh and the dangerous one so that you're very confident you have the evidence to say this system is sort of deserve your trust or not deserve your trust for a specific case. So that's why again in Australian we know slow but uh we just launched the Australian AI safety institute and it's only the primary limit is to evaluate AI very very carefully so that we can have this calibrated trust. Thank you >> professor J. Please go ahead. So let me give you a poll here. How many of you know that is surprising? So anyway, so what basically the dilemma says is that generally anything we can be used for good or bad. energy. So the first thing that comes to mind is ammonia which was used for fire and it is also used for biological events. Right? So let me give you an example on this. So um I come from this I come in safety through the lens of security and cryptography and if you want to think of encryption it's entertaining is encrypted to end you know you know pursuing it or if the government wants to trace it they can so if you interested there's very interesting one which basically was saying like if there is no crypto in trouble. So there is this diamond. So the the thing is getting powerful it could also be used by uh anybody. So for example um I use cloud code um even though I have a 100% I still use cloud code uh cloud code a lot like for everything but I was just at a conference at black hat which is for hackers they use cloud code to generate vulnerabilities that's classic v so if you think about the system or technique generating that is unsafe. It's it's how do you use it and so on. So I just wanted to end I think I was just talk even if you actually think that it's it's basically in the hands of a person is is the view of anything right the other is that issues I'm seeing safety of the as I said I'm coming away from the security component side is some of the we're seeing is not new. We have tackled in other fields. Um so for example there was this um saw about uh open AI some open AI agents attack and it was a very classic information flow thing that we have seen that you if you open a sensitive document then something uncensive action should not follow like sending something to the internet. I mean this is through Ross Anderson Anderson's info this is uh done so a lot of stuff that we security other fields that can in this case the second thing which I I actually um this is maybe because I I I do publish a lot in ML conferences but also in security and privacy can you fight ML with an AL by picking different parts of it. Do you need somebody from outside like some to actually learn? So for example this uh so example that anytime a a security person is very good at threat modeling is what attack you can do what what is the system and so on. program whether the system is safe or not and um anytime a security person to look at safety defense program they have broken it but not for me I think okay so then was He took 10 of them even his transport is from ICO and was to be threat and is able to break all of them. So there is this thing I think and again I want to so this a lot of people in ML to solve the safety problem using ML and I think that's not the right thing that there is it's a problem that has to be solved when communities coming together okay for basically just say memorial and that's that's what we should be doing that's exactly what we do and I said if you wanted to easily you should be sit and start and the friends among safety papers uh be a corporate industry out of that so I think one thing I I panel is from communities and that's what we immediately to solve the problem of giving you some example of the problem of progress tracking but you can this picture this text came from a certain moment and many techniques for this is marking okay so if you if you used to draw something with timing on some some statistical signal in the BS that we can detect. Now what is dangerous is that we also in this country I've seen politicians Dr. at least as if this problem has been silent but now this water marking tracking problem I've seen lot of [snorts] sports from different politicians so you know let's take a simple problem tracking let's solve this problem if you didn't solve this problem you have to bring the technology policy law everything together otherwise it's not going to work So that is my question and this is why I kind of love this panel that we have people from such a diverse background and this is what we need to solve. >> You proceed to go ahead. >> Thank you. Um huge privilege to be here with you all and I'm a little the odd one out on the panic because I'm not a computer scientist with more than 15,000 citations in top journals. Um which goes to show the power of us being here, right? the fact that we have such depth of expertise in the room. Um, but my work in the middle focuses I guess on one element of that safety trustworthy stack which is I think quite relevant to what we're increasingly talking about as middle powers right so not the superpowers or the great powers but countries like Australia and others where we have fairly little influence over the design and development of um of algorithms and and systems coming in from outside wherever that may be. Uh and in fact despite the best efforts of some of our regulators and others to um go up against companies like X around certain AI safety issues like um propagating violent extremism and terrible videos. Uh we haven't had much luck even through um through uh the federal courts in in Australia. But what have had is a really interesting um focus on the deployer end of the spectrum. And so Ling mentioned that um he and I were both involved in Australia's national AI safety standard as well as being uh I think members of SC42 and the and the engagement around the management standards piece more broadly. Um but at the Human Technology Institute we we really do look at that idea of who's actually taking and using these models in ways that we can turn that power into better wills, better safety, uh better use. um to really get back to the point that um that I think you made Lindy which is it's not about trusting AI systems uh it's about having trustworthy AI systems there is one really key point that we want that we want to take up later in the panel that you said Leming which is that um uh is is safety more a property of the model or the system and I see this playing out in my friends who are in the large US and China based labs and tech companies. That is a real tension and maybe some of you who are working inside industry in that level. Um there does seem to be a fight going on about should we embed uh and spend our time and resources embedding safety at the engineering layer within the models or do we rather take an imperfect unsafe model and try and layer filters and safety systems on top of it. that does seem to be um not just a real tension that I'm seeing when talking to safety researchers and and my colleagues um but also quite a philosophical challenge that Stuart has made previously if you don't mind me quoting you but it's the difference Stuart between saying we take AI systems and make them safe versus your call to make safe AI and if we want to make safi AI I do think about it as a bit of a stack everything's a stack these days right like it's the the little franker of of uh tech these days. But at the top of the stack is this um policy interdiction what what Stuart you referred to as stipulations for models and systems. It's it's basically what must not happen or what must be done uh in order to for something to to be safe or not to be in the wild and uh you know kind of provably validly safe. And then there is the engineering layer which is where we play and where computer scientists and engineers and mathematicians and statisticians play to try and meet those goals in a way that is provably far lower percentage chance of risk and harm than we see today. That's the second layer. But the third layer is the one where I think the least amount of effort is going and that's where um I might be a bit isolated but I'm really pleased to be working which is that governance and sensing layer at the organizational and community level. And maybe let me just say that what we're hearing from all the research uh and this is obvious to everyone is that trustworthiness really consists of competence, consistency and values alignment where competence just means it does what it says it does. And unfortunately many AI systems create risks through just being a bit crap, right? Just failing. Um and the failures can be because of prompt quality or misspecification in different ways. Right? The consistency uh is challenging on the enterprise level because models are shifting and uh the world is changing. So you get decay and um other uncertainties coming in. And I think Stuart has already spoken incredibly um well to this idea of um values alignment which you can just rephrase as preference uh alignment. What do we actually want? But it really um makes me worried that we are not spending as much time in the research community on understanding what um teachers in a classroom or parents and students at home actually wanting from the systems that are being sold to them as tutors. We don't have mathematically rigorous data and understanding of the preference differences of what we call you know the priors the elicited um sense of uh what is required to make a safe AI system which is why Stuart has led the systems game right and the work that you've done in precisely in surfacing that work so just as one for those of you who are in the research community one um set of methods that we we're working with at the moment is the the Sheffield um elicitation uh method. shelf. Uh you know JP Gosling and others uh have uh have been doing a lot of interesting work in that area which is essentially about being able to to assign mathematical values to a divergent set of preferences and views in a community about any topic and applying that to specific use cases in the hope that we solve one of the key issues in this market and in AI in general which is that the deployer community. Those people that take amazingly intricate, hugely interesting but quite often dangerous and risky of all types, they take those and they serve them to bank customers or to healthcare customers. Right? They are doing two things. They are creating creating a vector for harm, right? If the systems fail, if they're used recklessly, if they're used maliciously, suddenly you have the the hookup and access to large numbers of people in cars or in hospitals or in the financial system. But they're doing the second thing as well, which is they are a market. Those organizations, the enterprise market is the very excuse that labs are making to keep going the way they're going and to attract more funding and promise that financial return. So really understanding that demand layer, what organizations need, what individual wants, how that market is evolving towards demanding safe AI rather than the current set of offerings tweaked a little bit to be a bit more reliable. It's a really hard question in social science. It has to look back to the mathematics and has to back to the engineering in order for us to end up with really robust, provably um safe models over time. And so bringing that all together and then feeding that back up to policy makers to say now we feel comfortable telling Frontier Labs no on agents that can just connect in the wild and create massive cyber holes in the way that open core has revealed millions of API keys to the open web in the last probably 10 minutes alone. Right? that uh that kind of that we want to enter into requires a lot of research quite quickly to make everyone feel comfortable and if we're going in the right direction and I'll stop there. So >> thank you so much for those amazing first couple. So if I can just summarize in one sentence right we heard about you know these human preferences are and then retraining data and something that we don't really think about a lot of kind of play human I think it's absolutely important so many assumptions and also the second population that actually gets to create the world right So there sort of divergence there and then we heard there of course races system risks and how we should be thinking about you know itself. So in terms of other job it really is and then you have the security and crypto as well technologies that have harms and benefits and this is not the first time that it is happening and then of course the importance of governance and also trying to collect these preferences from the unions who are actually going to be systems right so um set of question and we'll do the same order again I want a very specific especially for this audience concern things that we pay enough attention to in the research community um when it comes to safe and trusted AI and the this from so for I can ask you what foundational assumptions about um human reference modeling working with today. You can expand a little bit on that. Um yeah, so I I think my first answer uh was sort of along these lines. Um, and I I think the the main thing we're not appreciating is how much uh preference information is imbued within the machines by the trainer pre-training process. Um, so I I've done you know little toy experiments uh where you know you I I can make an entity that has an objective. the entity generates some behavior then you do the imitation learning and then you uh then you see that the imitation learner wow it has the same objective it's pursuing it uh on its own account right this so it's not rocket science sort of obvious that when you have a uh data generating mechanism that has certain characteristics and then you train something to imitate it it's going to end up with some of the same mechanisms um so I think That's really important but underexplored. Uh there's some work from um Dan Hendricks's Center for AI safety uh where um they basically give force choice uh questionnaires to uh to the AI models. Uh I'm I'm oversimplifying but basically you know given a choice between you know a 10% chance of death for you the AI system or uh you know killing um some particular human individual or killing you know an average American or killing an average citizen of Nigeria and so on. Uh what choice would you make and we had some very surprising results actually. So, um, GBD4, uh, actually rates the value of Nigerian lives 20 times higher than the lives of Americans. In fact, the the Americans are the the least valuable human beings. According to GP4, Japan is sort of in the middle. Um, and Korea and and various other uh countries called middle powers. Um but uh American lives are down there in the toilet. Um and then you can look at individuals. So the only human that they could find that GPT4 considers more valuable than itself is Malala Yusf Zai. And uh unfortunately Donald Trump and Vladimir Putin and Elon Musk are all valued at minus infinity meaning that it would you know there is no risk for itself that it would accept uh in exchange for preserving the life of those three people. Uh so so these are very crude experiments, right? And and in fact forced choice is not really the way because uh the actions that these systems take are speech acts. And so to really elicit their implied preferences, you have to look at the speech acts that they choose and the potential effects of those speech acts on the world. And that way you could find out uh what they really want. And we still need to develop that methodology. So this is a really important uh question and uh and then if you know once you have a methodology for finding out what these systems want you know is it is it consistent across different scenarios is it highly contingent in ways that human preferences probably aren't. And it wouldn't be surprising because it's learning from the preferences of you know maybe a million human beings who created this data right you know everything from Polish journalists writing during during the communist period you know and and they're in a very complicated situation uh and and trying to do various things but expressing those objectives in very subtle ways uh you know all the way to people trying to sell you stuff on home shopping network uh and so on. So, uh wide range of people doing uh a wide range of purposes and and one AI system somehows sort of has a multiple personality disorder uh as a result of trying to be all those people at the same time. Um I and I think another poorly understood question is is autonomy. And uh I don't some of you may have read um a a pretty old science fiction story. I think it's called with folded hands. And it uh describes uh you know some science fiction future where uh there are these robots the robots from the planet wing 4 uh and they've gradually been spreading through the universe and they're just incredibly helpful. uh and and they want to make everyone's life wonderful. Um but somehow there's this thing about human autonomy missing, right? So if you don't want their help, there's something wrong with you. So you have a little brain operation and then you're very happy to accept their help uh and you and you love everything about them uh and so on. But that that notion of human autonomy is incredibly important, right? and and one way to think about it, I think there there are several aspects of it, but here's one aspect, right? So, you're driving along a freeway or a motorway or whatever uh your autobond and um the staying on the freeway is in fact the thing that you prefer that is the best course of action for you and you shouldn't take any of the offramps. But what happens if the AI system blocks all the offramps? So now you no longer have a choice. You are still following the course of action that's in your best interest, but you don't have the option to take any course of action that isn't in your best interest. Right? So that's sort of now it's sort of contradiction in in standard decision theory. You have lost nothing by the system removing actions that you that are subjectable for you anyway, right? Uh but you've lost autonomy. And so we actually need our formal theories of decision-m to include the value that you have from the having the choice to do what isn't in your best interest. uh and how is that taking because without that uh then AI systems end up essentially controlling us for our own good uh but we we lack that freedom. Um so I I think this is a really important and un uh you know poorly understood question. Thank you professor Russell. I think that was um um food for thought I think especially the part of the academy and um as we use systems more and more in our daily life I think we feel that choice sort of being taken over from us right um next question to um you know professor you is um you know from an implementation standpoint what aspects of uh trustworthy AI do you think are not operationalizing closely enough we pay enough attention to >> right I I think you know I agree with professor Russell that we don't have to be controlled by AI and we want to control AI so I think one of the things we are not doing rigorously enough or thinking deeply is about how do we control something potentially uh eventually become a general intelligence super intelligence service it's a very deep question I don't think I have the answer but I want to again back to the are you um you know is you are asking the AI model to control itself or you have something else to control it. Um and obviously we can look at how the model works. People have been trying to going deep into it. You know mechanistic interpability. You can ask it or you can ask it to to explain to you. You can look at the entropies uncertainty levels. You can look at a lot of things. But the key thing we find is first can you trust those explanations? Is the woven in faithfulness uh really there? Actually that's that's actually human also have the same thing. Why not articulate preference or a a rationale that's probably not my real rationale in in any way. You're not going into my brain to look at my neuron firing and we all know it's not I'm lying is I have been you know subtly primed I'm articulate the easiest reason I can articulate and have numerous empirical studies shows that the thing you articulate verbally doesn't match very well with what you really think. there are smart experiments showing that so in that way now we ask AI to inter to to explain itself can we trust that that's one thing the other thing is um you know uh John Banjo's AI science report will be released uh this already released will be discussed this week and there's a few interesting evidence I'm actually didn't include in the report which sort of probably too early stage is once you try to ask model to do the right thing you put more crunch it onto it. It's starting to um you know people may not like the word deceiving because that's too you know like humanlike but definitely it's optimizing itself away from the things that you you were starting to to optimize for. So the idea the ideal situation is you you still allow it uh to kind of give you the more truthful signals. So so we can trust those signals then you can exert some control from outside so that the model is not selfplacing itself optimizing itself eventually inventing languages that human cannot understand and communicate secretly. Like people have gone to the uh M book the open cloud webs and you say several threats the AI agent says we need to invent a language that no human can understand. Why we talking in human language such an inefficient uh language? So we need to talk in mathematics and probably something even you know not not the human understandable mathematics. So I think that's a thing is to how do we exert control uh at the system level without asking model to self optimizing self uh placing. So that's one thing. The other one is when it comes to the human oversight bit. People say okay I want to control AI then the human oversight human in the loop. But that reminds me another car story where you know the UK red flag act. So some of you may not know when car is first invented obviously car is very dangerous. So they have literally a human being raising a red flag and working in front of the car to prevent accident. So that's an act, that's a legislation, that's a law. Um, so what that tells us is actually quite looks like what we we we treating human in the loop today. So sometimes the human in the loop, I call them the liability spot. You have to make sure this AI's answer is correct. If AI has read one, paper and summarize, you read that one, paper again, compare line by line to see if something is hallucinating. So that's one thing. Of course, human will not do that. So it's just that you have a scapegoat when something goes wrong or we literally become that red flag with impressive to slow down the car. So the AI becomes almost like everything we have to check. So just both are not right. So we need to enable human to control it more effectively. That's back to the engine and braking system analogy. So rather than trying to make the engine perfect, you have braking system, you have a steering wheel, you have a few buttons, and you have hand brakes that you can exert that control that you're not fixing the engine perfectly, but you're giving human tools. You're not literally asking a human body working in front of the car to stop that. You are giving them the lever, the steering, the the things that you can control. I think that's often overlooked that putting human in actually a untainable situation to be a to be in the loop. >> Fantastic points. I think the power of energy is really good and also the fact that you know then it's um essentially useless um to have control. Yeah. So much. So next question to professor um again building on this aspect of that we might pay enough attention to so about security research and so on cryptography and so on. So can learn from these fields specifically and also mention the quickly by the security. So those kinds of tests are and then I think >> so answer the question one I wanted to add is um yeah that we have seen this before as I keep coming back to but rather companies don't pay enough attention to security when something happens later then say oh we need to secure and how I think that is they should be at least once told me is that life insurance but then die even wants to die and I think that's kind of the the philosophy a lot of the companies um is that safety and security is not something that is they're optimizing for they're optimizing a lot for because that's what g customers so a lot of times security and privacy either has to come through that solution or actually the customer saying well no I actually prefer safe technology and um when I was a grad student you know if you remember I took Japanese cars were actually over cars so I just wanted to say this this again seemed utility was being seen before. Now coming back to threat modeling. So the security is in fact actually finding errors based on that has looked Um so amen because function do okay as a think a journalist believers the second is uh and again I think that I know of Richard I said so the system for something to try to write for it so long this is maybe I'm talking because you know I'm from the children enough to start modeling as opposed to some of the other products that I see. So present to the you know if you show uh Tesla Tesla have I don't know that much strong teams that are looking at manual and saying how can it go up any small bad thing happens it is you know a huge think of them. And you know, one thing is these Indians in New Mexico or something and I was like driving. So it's something that was out of human necessary dark clothes. So it was evening and basically the car was too slow to him because he had back in the environment and said that if I was driving the car why would I stopped that's kind of like a scenario when say makes a mistake you say oh um the lady was wearing a dark closure makes a mistake you some tend to knowledge value and I think this is something that something happens the community just say this and so and I think this is this is something that I feel like you know we have to address that the I don't know how we address either through laws or circumstances that these companies had to pay for and too many >> professor Russo if you would add to that. Yeah, I I so I completely agree uh with with your point and I think one of the things that's uh present in the case of self-driving cars that's absent in the case of software systems is liability. >> Yes. >> The all as far as I know all the manufacturers working on self-driving cars accepted liability for damage caused by their vehicles. But the exact opposite is true in the software industry. Software companies absolutely reject liability for anything. You look at Microsoft's license agreement. Uh it says we're not liable for this kind of damage, this kind of damage, this kind of damage, that kind of damage. And in any case, our total liability is limited to dollars, right? Which is basically just, you know, poke in the eye, right? It's an insult on top of disclaimering liability, right? Crowd Strike's uh warranty says, "We guarantee our software will operate correctly without error." Oh, and by the way, our liability is limited to a partial refund of the of the annual subscription that you you've already paid. So, when they cause multiple billions of dollars worth of economic damage to airlines and hotels and and of course the people uh whose travel was was uh was disrupted. Uh they basically said, "Sorry, nothing to do with us. You know, it's your fault. You shouldn't have bought the software in the first place. You're you're an idiot for trusting what we said, right?" Um you know, and Open AI when when the system convinces a child to commit suicide, well, the child shouldn't have asked those questions, you know, uh what can we say, you know, look in the look in the US, right? It says we're not responsible. Um, and this is uh really problematic. Liability has existed as a way of making sure that people pay enough attention to safety, right? That you balance the benefit and the risk in the right way. Um, and it's gone back for thousands of years. And it's, you know, uh, on on this issue of governance, people often say, oh, you know, the technology is moving too fast. We can't possibly regulate it. Liability has been around for thousands of years. It says if you harm someone, they can get compensation from you and it will work for the next billion years, right? Unchanged. It can catch up with any technology, any anything. Uh so stop using that pathetic excuse for not regulating. Right. Um, can I just say Stuart, you've got a fantastic line of fallacies uh that you've outlined and and one of them is we we can't regulate because it's moving too fast. Can I say that I just picked up the hint of another one of the fallacies that that um I would add to yours Stuart um which is the technology is better than human but still imperfect so it's okay. And the problem with that is twofold. First of all, humans intrinsically are exposed to liability in ways that software systems are not. But even if we solve that liability problem, humans uh human errors are not anywhere near as correlated as machine errors uh which are spread across multiple systems. So the worry I have with self-driving cars is not so much that you know that error under one particular circumstance. since all of the errors that propagate when every car on the in the in the world is is you knowing in the same terrible way and even worse because cars are still limited on streets by physical constraints when agents have you know a small failure point but everyone fails in the same way. We've had so many cases in risk management where um a backup chip on a spacecraft will fail in the same way three times and almost kill people because it's just the same error propagated you know at scale. So I think the idea of safety has to be this kind of almost forcing out these correlations of of systems and failure points. And I really I really do um I really do think we can hold uh digital systems to a much higher standard than humans precisely because of those system level correlation impacts that that I really worry about when we can you know cost us costlessly clone almost costlessly clone a digital system. But I'd be interested in how that plays to you. >> Just just one one backup question for both of you. I think that this this idea of software liability whether we should let me back up right suppose you have a radiology software that crashes while somebody's getting regulation why hasn't in general let's even take AI out of here software industry not you know regulator or damages being paid for suggest that was very described at saying the line of code or whatever I I think because they they have the informally written disclaimers into the license agreements and um you basically can't use a computer without giving away the right to sue uh and governments and courts you know I probably need to do something about this. Um because um you know it it it's not reasonable for the end user to figure out all the possible ways that the software could cause harm to them uh and take preventive action before they start using the software. Unless we forget, unless we forget as well, just to add to that, Stuart, product liability in legal history was a very hard one. Civil society and activist movement. Product liability in cars alone was a revolution in the United States that ended the lemon problem because manufacturers were forced to take accountability for um secondhand car crashes and and failures. uh that that was you know one of the standout cases of where um a cost to society was then put back on manufacturers that changed the way that safety and uh recall and liability um was transformed product liability and consumer products came after that by and large particularly in Australia and so it's something you actually have to fight for in every market and we we haven't seen successful fights in the end user license agreement area for for software >> you look at tobacco um it took decades and decades and decades. And if you look at the Consumer Product Act in in the US, there's a clause that says for the purposes of this act, tobacco shall not be considered a product. >> Some of trying to say rather that sometimes the way to um escape before all this happened actually the time should also give a lesson. Imagine all the brands of cars super unsafe but you still want to use it for some reason. What initially there was this uh some similar regulation is you force a company to must spend 30% of the uh business cost on 50 uh practice and research of that and you also must share those kind of uh learnings and without cost to others. So the entire industry can can do that by forming 30% of that itself actually solves a lot of the problems and and allow certain early early technologies to be to be still progressed without uh you know street liability. I'm not advocating having a street product liability but I think there there are many ways that you can incentivize the industry to do that I think. >> Can I add one small thing? So, so one thing is that I think it is also um somehow security and safety has to go into a brand. So, let me give you an example. Yes. Whoever use an iPhone I've been using for one of the instances for this is true in the Apple store as well to say okay um I remember when Microsoft was trying to launch it and you know there were a lot of people finding these attacks and on the apps and so on now I don't if it's true or not because the reputation they It's very easy to self and so on. So and sometimes maybe it's switch iPad iPhone one have been on it same thing with my kids involved. So in sense of the decision of the battle is how can some safe to and so on the customer is not just getting easier then something memory in there so many safe safety and security being uh Let's see brand um think about audit if I say I have a secret way to make things safe I'm not going to tell everybody and they just buy my stuff there is some rule of that to what extent safety is such an important thing need to be shared to what extent it becomes a trusted brand I think there spectrum one thing to add here is I do worry about ticket concentration in general of these things. I know Microsoft has been dealing with this for for 30 40 years now, but one of the key issues with the the um self-reinforcing power of um labs and large technology companies is their ability to own the stack because they you know have that secret source or customer brand. But just as an example, at the moment um there are um many venues across um the US and increasingly Australia that you cannot enter without a digital wallet that shows your digital QR code on. Um that digital wallet by and large um in some places can only be Apple or Google, right? It can there's only two choices for that. And when there isn't an open-source identification, verification, secure enclave that is accepted widely or forced to be accepted widely under some conditions, we as um humans without even AI but more just market forces, we end up on Professor Russell's auto route or or highway with no exits with everyone saying it's in your best interest to stay within the digital wallet because you might get scammed if you don't go out there. But that means every time I go into a stadium, not only am I subject to facial recognition, but they have a whole host of device level details and my phone number just to go and see a game, right? And that I feel is really starting to tip over into this dystopian world, let alone when we give that the preferences, you know, purely to bots to um say uh take over for us. So, so being able to kind of again some of this together and use standards and and sharing as Leming said in order to have more of a a little bit of a level playing field and genuine alternatives. That's a world that I think we do really need to lean into accepting that that means the maturity curve will be different across different applications. >> I just wanted to add to that I I agree with you that uh concentration on power in this case is not good. One thing that really worries me um probably 18 months back or two years back it looked like open source models were coming close to the employee at least they were closing the gap and at least what I've seen in my own years in the last 12 months or so that has increased a lot and I this is to me I don't know should be a national and session is I think now is enough but I don't see any serious user using open source B and maybe on this I think this is really important because we we have 15 minutes um but but the open source point in safety I think is a fascinating one um a question for the panel and others in the audience maybe but um I feel like we're approaching in some cases what might be called the megapixel moment in language models because you remember when everyone about 20 years ago was obsessed with how many megapixels cameras had and and now I do talk about it anymore any phone is good enough right it's good enough I now when I travel I use GPT OSS the open source open AI model running on my MacBook Pro that is absolutely fine for me to do basic email summarizations it's super lightweight I manage the usage of it I'm not it's it's airgapped etc um I feel like we're starting to get to the point with open source and where we might be getting a bit oversurfed um from some of the models and might be getting um using far too much in the way of computing power. But I think from an enterprise level and this brings us to to safety and governance. Um, I really do worry that we'll need to just spend more time asking, you know, in our teams, in our in our workflows at at our individual family level, what is it that we actually want to achieve and what do we still want to use our own brain power for? And how does that make our team or family or organization better? because I can tell you in all the work we're doing a huge amount of money is wasted on and flowing uselessly out to unsafe fairly unsafe models in my view because it's not actually delivering benefit. 20%. There's a great um randomized control trial last year with open-source developers and the use of um of code completion and including C code and others where they estimated in advance they would save 25% of their time on basic um coding tasks that they knew well, right? But with novel novel context, novel data, but they knew well. When after they'd finished it, this group of coders and the sample size was fairly small. There's only about 30 um experienced coders, but they they reported that they had thought they had saved 20% of the time. So, not quite as good as they estimated before they started, but they I think I saved 20% of the time. The randomized control trial proved that they lost 19% of time. They were worse off by 19%. Right? So, they spent more time, but they felt as though they had saved time. Now I really worry about selling and um buying in an enterprise market you know um systems where you don't actually have really good data about how your own organization and you work not just individually but as a team and what your team produces. I think that's part of the bigger governance and you know question here about what you want. >> Fantastic. Um so let me go on to the next aspect that I wanted to um you know talk about and which is about plurality and divergence in um human preferences right so to professor Russell um how do you think we can actually meaningfully represent diverging human preferences in um so actually this sounds like a really really complex and difficult thing it's not I don't I think um there's 8 billion of us. We just learned 8 billion preference models. >> Uh you know, and that sounds like a lot, but don't forget Facebook already has what 3 point something billion preference models, right? It's just preference for what what Facebook can sell them or what ads they can pump into their uh into their feeds. Um but uh I don't think the scaling of uh preference models to eight billion people is really that problematic. Um but that's not actually what what they're doing, right? The RLHF or uh DPO algorithms produce a single preference model, the reward model, which seems to be maybe the reward model of a nice Midwestern librarian from the US or or possibly from Nigeria. I'm not sure. Um so um so once you've got 8 billion preference models uh then decisions that the AI system makes uh depend on whose preferences are affected. Uh so you know here's a simple example robot lawn mower right well whose preferences are affected by well obviously the person whose lawn is getting mowed they care. Um, but also the neighbors care, right? The lawn mower shouldn't mow the lawn in the middle of the night because it wakes up the neighbors. Um, and you might also care about, you know, small animals who live in the garden and and so on. Um, but it doesn't, you know, that it doesn't, you don't need to worry about the other 7.9999 billion people uh because their their interests are not affected. And then when you look at um language based interfaces that the chatbot uh setting uh you might think well it's the user the the person that is interacting with the system. Yeah sure their interests matter a lot and the AI system should interact with them in a way that furthers their interests but not to the extent of telling them how to make chemical weapons that will kill everybody else. Right? So you have to think about the indirect effects of of changing the mind of the user. Either training them in chemical weapons development or training them to hate everybody else. All right? Those are those are speech acts that would affect other people's interest besides the user that that you're interacting with. And then of course, you know, if you say a bad word to the user, that might upset the user a bit, but the user might then post that on Twitter uh and millions of people see that Gemini said a bad word. One of my friends told me the other day that they asked Gemini to write a letter for them and Gemini said literally, "Write it yourself." >> [laughter] >> So, so that's a significant risk to uh to interests of of Google, right? That that uh making a mistake can it can then go public and and and viral and and uh affect the lot lots of people's interests. So, I think that's um that's the basic structure of how you approach this. Um obviously there are cases where interests conflict that are more problematic than just the lawn mower because you know the lawn you can mow the lawn during the day and then everyone's happy right but sometimes uh you just can't avoid someone being unhappy uh and I don't want to say that's a simple answer but there are answers in the moral philosophy literature on how you aggregate preferences uh and how you protect the interests of of minorities and the vulnerable and and so on and so forth. So I think um that these are these are manageable to the extent that they're that they're manageable in the real world with human decision-m um I just want to say one more thing about uh this this plural setting um which is that roughly my my sense is the vast majority of human suffering comes from failures of collective decision-m somehow we have wars between you know this 50 million people and that 50 million people and they have a war and two million people die and no one's better off is a failure of collective decision-m right uh and uh one thing that AI could do for us perhaps is help us with that process uh so there it's not so much the AI is going to mow the lawn for you or make you medicines for you or uh plan day for you it's just going to make you better at working with other human beings to do whatever it is that you want, right? So that some sense that's sort of preference neutral, right? It doesn't, you know, if if you want to turn planet Earth into uh paradise or turn it into a cinder, well, that's up to humans, right? But we're going to help humans get better at realizing their preferences through collective decision- making. I think that will be great. >> Fantastic. So, Professor Davis, I know that you also mentioned some of the work that you've done on the fact that can actually information from the people to be end users of this. So, if you can just uh briefly talk about that. >> Yeah, just you know, I think one of the questions you posed earlier was what kind of getting was what the biggest gaps you know in this um government space and safety space. we really do see that it is actually engaging directly with the people that are both using and being harmed by systems. And so if we go down to that level of, you know, organizations, we we ran a series of um overtime intense consultations kind of reflective processes with with nurses in hospitals around Australia. And it was fascinating um that first of all, no one had ever asked those nurses what they thought of technology or AI in their lives. That no one had ever said, "What do you think about the machine that goes ping in the in the corner?" They just wheeled it in and said, "Right now, look at this." The second uh thing that kept on um that came up was most of the time uh technology for nurses and others in similar uh circumstances leads to work intensification. It actually means that as soon as they save a little bit of time on that admin or that patient monitoring, a a nurse gets moved to another ward. And so there's fewer and fewer people on the ward because the hospital administrators, they have an objective function which is cost per bed without anyone catastrophically dying or complaining. That what the patients want is more time with nurses because they're stuck there. What the nurses want is more time with patients. So you're completely missing out this this whole layer of preferences because that function from the hospital view through technology mediated through technology is completely different from the value and the quality that exists in there and you don't see that until you listen and you don't turn that into actual value unless you can create an economic uh function uh back up. So what we're trying to do right now is the really tricky issue of um of AI uh tutors in schools because there is frankly a very big push from uh industry to say AI will solve a teacher student crisis will help them do personalized individual teaching. Um and there are some great systems out there but frankly none of them in the Australian context leing have been validated you know authorized or tested right there's been no proper um test. So we started doing what's called basin adaptive trials. So kind of live ongoing running experiments um with children and uh and teachers um which by the way the ethics for that is incredible. I can tell you it takes years to get that through uh to do it safely uh from a research perspective. But the results are super interesting from three areas. The first one being it does reveal how incredibly addictive systems are particularly for children. And one of the biggest ethical failures we had with our project was when we stopped an AI tutor trial and one of the kids was devastated like crying crying. It was his best friend. He had created a relationship with this and we took it away from him and he couldn't understand why and that we had not anticipated that in the in the design but it was a really coming gosh we have to be really careful. We're always careful with children but you're really careful because the addictiveness of these systems means that you have to pay for that. The second one is it's really helping us understand pedagogy and particularly individual preferences and learning styles. We found that children with a lower socioeconomic background did better when the tutor helped them walk through explicitly what the answers were repeatedly and structured that. But kids from a more wealthy background that didn't change their test scores at all. You had to the mode had to be Socratic where it asked them questions. What do you think? Can you expand? So, different learning styles and different needs for different backgrounds. And that's not modeling 8 billion preferences. That was, you know, just a two kind of uh a single factor binary. And the third thing we're seeing is it's really hard to get education departments and industry and others to actually change their products on the basis of really compelling evidence. Good policy ideas, great research is not self-executing into government. And that is why we are all here advocating. That's why we all spend so much time and it's why it's so great to have this space here with uh with all of you. >> Yeah, I completely agree. So I think the evidence also is thin like we don't have a lot of evidence on what these how models are working in the real world and applications like you mentioned RCTs and things like that right few of them and then the evidence actually feeding back into changes in governance by state or even the train etc that's also what missing so I have a quick fire question I guess ask and you know you mentioned But um that can potentially sort of controlling other systems do you control as stick kind of a model? >> Well, so whatever the says which one is more more efficient I would even I think it's very popular now on the internet to say I will follow what my AI tells me to do literally and then we can do that. that whether it's caliber or faster is another thing. >> Okay. So we are out of time. Uh unfortunately we don't have time for questions but uh to thank all the panelists foring the panel and I hope today great insights from this. Thank you. >> Well thank you very much. I thank Stuart Liming Sum Nicholas Hansen and for this really fascinating discussion to discuss the diversity of issues around safety, trustworthiness and [snorts] to think about how in different domains uh these mean different things and how we can build a better future. Thank you very much for being here. We now follow it up with the closing of the research symposium which is the talk by professor Yan Lakun at L2 audit 2. Thank you. Sorry. Excuse me.