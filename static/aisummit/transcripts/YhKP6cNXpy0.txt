infrastructure. We have um Mr. Akash Kapoor who's a senior fellow New America and Gov Lab and also a visiting scholar from Princeton. He writes regularly on tech policy for the New Yorker WSJ as well as NYT where he is a former columnist. We also have um Mrs. Deepika Magnusetti fromstep who is going to be our moderator for today. Unfortunately, two of our other speakers uh could not make it. Um >> and yes uh Mr. PK has is a professor from IIT Hyderabad and so glad to have him here. uh if you if you all can just step forward to just take a picture a quick picture and then um we can go ahead with the discussions. Thank you. This is a nice cozy group to have a conversation on safe AI. Uh I'm sure we've been in many panels over the last two days and it must be a little bit exhausting. >> No no I came on only today. >> Excellent. >> From Hyderabad. >> We will only pick your brain. Thank you for joining us. Um well, good evening everyone. Um we're here because we've certainly reached a certain point in this conversation of safe AI which is moving certainly beyond principles which everyone has agreed to and happy with to what does it really mean to bring it alive in a real way. uh moving from principles in some way to the conversation on what does access to safe AI mean and also what does safe infrastructure mean safe a safety infrastructure mean fundamentally good intentions don't scale so as we build population scale AI how should we think about safety what should that infrastructure look like is the conversation we want to have today uh we're very happy to have Akash and PK is that how we should refer to you >> yeah yeah that's People know me. So >> do your students call you PK? >> Sorry. >> What does your st What do your students call you? >> Uh PK. >> Yeah. >> Even a first year student calls me PK. >> All right then. Akash. Let me start with you. You've studied how transformation technologies shape communities and social structures, right? uh when AI safety principles are developed in labs or in policy circles and we've been in many of those rooms what typically gets lost in translations when these systems encounter real world complexity basically intent is great um when you hit the when the rubber hits the road what gets lost and what's difficult to implement and bring alive >> um yeah thanks Thanks. Thank you all for being here today. Um I mean I think that uh you know as you say when technology moves out of the lab or out of the creation phase into the world uh it encounters society and the complexity of society. Um and you know it encounters users and all the sort of complexity that users bring with it all the kind of uh demographic and cultural complexity. So I I mean this is not something that's really new to AI, right? Like I when I was writing my uh PhD, there was a lot of uh talk about ICT 4D. I don't know if how many of you remember that it was information kiosks and and I I still remember you know doing my field work in villages around Pondicherry and in Tamil Nadu and you had all these information kiosks that were supposed to democratize information but then it turned out that like where because the villages were delineated by cast so where it was located in the village had a huge bearing on who could actually access the information. So I think with AI we're going to see the same thing. we're going to see that uh you know on the one hand there's a promise of this kind of like all of human knowledge embodied in these models but then there's a question there's all kinds of questions about access and and how they're actually deployed in the world which will be filtered through culture and society um and all of that and I think this is a really important dimension of safety I mean we've obviously moved past the notion of safety as just being existential risk these sorts of like exclusion inclusion factors are part of safety um the one other thing I would say is the other uh aspect of safety that I'm um increasingly interested in or just kind of access is the temporal dimension because we create technology, we release it into the world into a particular configuration but then the set of incentives and the configuration changes a lot over time. Um there's been a lot of work on this relating specifically to the internet and how the sort of you know the internet was originally this kind of like open platform uh and over time it became a series of walled gardens and the experience for users really changed. So I think for when we're talking about safety uh and measuring safety we need to think of it not as a one-time kind of check but as an ongoing process um rather than a kind of like you you don't just create technology decide what that it meets certain safety standards. there has to be sort of ongoing audits, certification, whatever it is uh temporally. >> I want to follow on question actually. I find that the term safety is now being used for so many different things. It's becoming literally a safe word. Is there is the word nearly good enough to just describe the complexity of what you have just put on the table? is is is safe becoming too um so allencompassing that we're not able to actually capture the nuance >> or is it still the I'm asking you. >> Yeah. Um I I think you're right. I mean you know when uh when safety started like when AI first sort of burst on the scene safety was almost like defined as existential risk and then there was a lot of push back against that because we realize that the before humanity gets wiped out by AI there going to be a lot of other problems that AI is going to so I I mean I don't think that safety is actually the right word. I think we're probably grasping for another word. I mean one way to look at it is responsible AI and what does responsible AI encompass? So I think of like the various risks or disruptions or harms that could be caused by AI and for now we're just using safety as a kind of catchall term. >> Catch all term. >> Um TK the question to you is your work in cyber security privacy all of these um social media safety in the Indian context. when we talk about AI safety at population scale in India uh across languages what are the unique challenges that global safety frameworks we're coming back to that word but global safety frameworks often miss >> yeah um I think first uh we first let me get a sense of the audience I think according like so how many of your students uh professionals working in companies uh anybody else how many if you don't know what you're doing okay very good that's a very good state to be in um so I think when we discuss uh this whole safety frameworks everything right um uh I think India generally is looked up as oh this multilingual problem Telugu Tamar Marati and Therefore, it's uh it's hard to bring in the safety uh hard to bring in the responsible AI all that I I definitely that is one of the problems. Uh that is one of the big uh questions if you have to actually get all these technologies at scale technologies at uh population scale deployed all that. uh but there's also one more I guess added to the language itself which I feel u so how many of you speak two languages English and something else okay how many of you speak three languages four right so some of five right so India also has that problem right I think generally when we talk about uh India we end up talking about oh this English one probably next is Hindi probably and then we end up stopping. uh there is also this problem called code mixed which is I'm I'm pretty sure all of you if I ask you to open your WhatsApp and see uh from the morning the messages that you have sent there'll be at least once or more than once you have switched languages between uh switched between languages evening dinner we'll have dinner together right uh evening traffic I'll reach airport in 10 Right. I I switched between languages. So I actually don't know what the solution is. I actually think that for population scale uh safety or responsible AI solutions in India uh this is going to be a next level problem. I think we can I think we know how to solve a little bit of Hindi, Marati, Telugu separately now but I really don't think so we understand anything about this code mix direction. My follow on question to you is that so then when we talk about safety benchmarks right what does it even look like hyper localal context context driven who sets it how do we do this how should we think about this >> uh so I I also don't know a lot of answers to it but I know actually the efforts that are going on across the country probably that can give us a sense of what are the solutions that we should do for example uh has uh uh deployed some solutions in farmers in different parts of the country and uh when you look at uh these solutions these are basically chat bots here farmer is asking a question saying oh what pesticide should I use uh the weather is this what uh next what should I try oh I have this one acre of land uh I just finished ground net what should I do next all these kind of questions they can actually interact with this chatbot now This is what asteps deployment is. Uh but there if you see there is a lot of questions about safety itself. I mean meaning somebody can ask a question. For example the the there was a demo that was shown like a couple of months back where uh somebody asked this chatbot saying oh I want to actually write a petition to the chief minister of Maharashtra. uh can you write a draft of a petition and the CH GPT nicely wrote a petition and gave it a CHP the the chatbot uh wrote it and gave and it was also very negatively pitched and all that so is that right should the model be doing it so that chatbot should have done it all that so I think that's where India ecosystem um uh deployed system at scale if you were to attack the safety questions this is the kind of problems that we'll end up having when when it goes to and particularly in India I think we are we are deploying many of these technologies to people who are in the bottom of the pyramid also right top of the pyramid there are problems probably we'll fix it we'll figure out something but the bottom pyramid I think it's going to be extremely hard because of um uh some of these interactions are not happening in typing text it's happening in voice we don't know how to understand voice again for all languages or multilingual and all that yeah that's my take right now >> that's actually a very good example to lead into the next question. Akash and the question really is safety at scale requires institutional accountability and there are so many different types of institutions involved in the deployment of any solution. Now as I mean government is one when it deploys it but there are also private things all their interests are very um varied. Mhm. >> How do you then uh create frameworks that manage these different incentives, interests, pressures? So maybe the fundamental question is whose responsibility is safety and how do you create institutional structures of accountability? Can it all can it be created at even? I don't know. I'm asking. Yeah, that's a I mean a very tough question that seems like a kind of political economy question. I mean I think one thing I would point out is that sometimes the incentives don't always line up in the wrong way, right? So I think we we tend to focus on the negatives but there there are positives where um you know tech gets created in a kind of open way and so and and in surprising ways like I I wouldn't have predicted that uh competition in AI and geopolitical competition in AI would push towards openness in models right like so open models have their own set of problems I'm not saying that they're the solution to everything but um particularly after the deepseek moment uh competition started pushing in the direction of openness and the U a lot of US players had to jump on that and so I think that's like a really interesting sort of alignment of incentives. Um I think another one that has gotten surprisingly little attention but I think is like a really big deal is you know the the way Anthropic donated MCP to Linux. I don't know how many of you how many of you are familiar with MCP since we're doing hand raising. Do people know what MCP is? Sort of. Yeah. So I mean you know MCP is it's like an open protocol basically for agentic AI that allows agents that would allow it create a common language for agents to talk across uh gates across closed gardens right that would allow it's like think of it as kind of like almost like a TCP IP for uh what AI could be and um you you could have easily imagined a situation where this would remain highly fragmented and your uh agent from one company would not be able to access a service from another company but it turns out that like I think companies basically realize that it's in their interest to build this ecosystem. So Anthropic created this MCP protocol donates it to the Linux Foundation effectively creating a new public good. So and I'm just starting by saying that sometimes the incentives align in interesting ways. This is where public interest and private interest kind of aligned. So when they don't align which often happens I think that's where you need public policy and regulation. And you know typically we failed pretty miserably on the internet with this um to kind of like give more value to the public good over the over the private good and and sort of private uh profit profit maximization but we we we now have the benefit of hindsight and so I you know my maybe it's an idealized hope but I would hope that this time around regulators and policy makers would be more aware of it. Um so yeah I mean that's it's it's not like a a golden sort of answer but I think this enters into the realm of regulation and policy >> and therein lies the question in terms of um what are we doing about the thing you talked about past lessons learned right um if we didn't know what we were doing before we know even less now particularly because of how fast things are moving >> right so In this scenario, where should accountability? How do we think of accountability? There are the developers of certain things. There are the models. Should in your mind and if you have paid any thought to this, what kind of um distributed accountability structures which are very different from trying to ring the neck of one person who you can catch the opposite of that. Are there any ideas there that are coming to your mind? >> So first of all I would say do we actually know less? I mean it we we know less in the sense that maybe we have these black boxes and tech is moving faster. But I think we know one thing um that we should have learned from what's what's happened with the internet over the last 30 years, which is that if you just let tech and private interest in tech take its own way without policy and without regulation, you're probably not going to end up in a very good place. And I think that um you know around 2015 2016 when this started becoming apparent, we could say well we didn't know, right? Like we didn't know it was going to end up here. But this time if 15 or 20 years from now we're talking this way about AI, we won't have that excuse, right? So I think that we we do know that this is where things could go. So in that sense we actually know more. And so then the question is like where does the accountability lie? Um I mean I would say that it lies in this is a this is a democ this is a question of democracy, right? These are these are these are not fundamentally different than questions about decisionm in other areas of democratic life. like how do you get participatory technology? How do you input public will into technology? There is an argument that well you know this stuff is so complicated. How can how can how can the public be part of it? How can you have public participation? It's true. I think there's something there. On the other hand, democracies regulate highly complex areas like pharma, nuclear safety. I mean so, you know, I don't I I don't think that this is unprecedented. Um and I think the job of democratic institutions is to channel public debates about values like do we do we value um individual freedom or do we value equality collective equality? These might be in conflict. How much do we care about efficiency and speed um which might be in conflict with with you know justice or things like that. These are democratic questions that hopefully can be translated through the democratic system into how we regulate technology. >> That's interesting. So what you're really saying is this is not a technology problem as much as going back to foundational principles of citizenship and democracy. And if you don't have those foundations that are strong, then you really are not going to be able to build um or or at least ask for safer systems or hold to account for safer systems. >> That's essentially what you're saying. >> Mhm. >> PK, I saw you nod your head. Did you disagree? >> So do I. I completely agree. I was going to give examples of privacy and things like that. I don't think so we understand uh how to do some of these things in as he said topics if you go back right I I don't know whether we know to you how to uh who's accountable for things on the internet we don't have an answer 60 year old question and then you talk about privacy. Yeah I think it says the same. >> Yeah. >> So >> can I just say something? I mean I think privacy is a a good example because I mean citizens might be willing to give up a certain amount of privacy in order to have certain benefits right like this has been a common sort of debate around around Aadhaar and I mean that that decision that is a public decision that is a balancing of competing public interest goals which democracy should be able to handle basically. >> Yeah. So then the question given particularly you're talking about harms and uh privacy and and that question in resource constraint settings pretty much like ours where our experience has been regulatory capacity is limited. So and and if you look at also the response currently in terms of how we're looking at AI as well, we're actually treading very carefully with guidelines and things that way much more and not jumping into even trying to legislate this. >> Right? So in your mind now that you know what you know and you've studied what you've studied what could potentially be lightweight safety mechanisms for the kind of systems that we have because you do know what the capacity looks like. >> So do you have any ideas on what it might look like from a regulatory framework if that's even the right word? I don't know. >> I I actually don't know also but let's let's go ahead with the question about so h what would it look like right? uh if we have to I mean accountability frameworks I don't think so we understand these topics very well in many other domains similarly probably in the space of AI also >> um but I think I mean there have been let's let's go back and look at some of the approaches that people have tried for example uh security privacy there was this framework called self-regulatory right uh people tried I think India also data security council of India I just saw somebody from data security council of India here. So uh organizations would sign up saying that this is what they would do this is what they would not do but there is no regulation. I as you rightly said before we are limited in our impact influence in these frameworks if you were to say um I think there has been some efforts in the US also starting to come or tried in this self-regulatory but I don't think so uh it is uh it is very very effective. So what I'm trying to say is that there are methods that people have tried in other areas in the past. I don't see it yet influencing the AI space. Even if it influences, I don't know where it is going. >> So in that scenario, what does meaningful safety and responsibility look like? >> So in in my head, I think the u u I think companies uh guard rails around that that's probably one approach. The other is of course uh user safety right. Uh so here is a simple task that I uh can do here. So how many of you have downloaded a single app in the last one month on your phone? Raise your hand if you have downloaded one app. Keep your hands up. One app in the last 6 months. Essentially I want all hands to go up. One app in the last one year. Okay. Keep your hands up. Keep your hands up. My questions are not done. One year. Right. When you downloaded the app, there was this thing called as agree. Right? How many of you read what is the text and then click the button agree? And how many of you did not read it? If you have read it, please keep your hands up. Did you write it? Usually that's the case. People who are keeping the hands up are the lawyers or somebody who actually write it only. They are the they're the only ones probably look at that. No. So same thing is happening with all the models or all the AI so to say tools that you use. I'm sure you all signed up when you logged into charge GPT. You gave something uh you don't know what you gave and so now what can you expect right? You gave something to Facebook when you signed in. Uh so I think that's the I I see the world in the same way like what I saw like 10 years before in social networks where we were downloading the apps installing uh them and end users license agreement. and we don't we did not care about reading it. I mean I think old saying right the the problem is just I I'm reflecting on what he said before nothing has changed it's the derivative which has changed in the AI which is the quickness fastness impact that's the only thing that has changed last 60 years at least the things I know of >> can I uh just yeah I I agree with that >> and um the only thing that has changed or that's trying to change besides the speed is that somewhere along Ong the way tech convinced the world that it shouldn't be regulated, right? So like the I was thinking as you're giving the example of the consent form which is very important. Imagine that 100 years ago whatever the pharma industry had convinced us the same thing. When you take a pill, you have to read a 30-page medical document and say whether you agree to take it or not. That's not how it is, right? We rely on regulators to not put pills out in the world that can destroy our lives and kill us. So somewhere along the way tech convinced regulators that it shouldn't be regulated and offloaded the regulation onto consumers. Right? So I think part of the challenge is to like to take that back and give give some authority back to the state um whether whether it's through tech specific regulation or existing regulatory frameworks is something you know we can talk about. >> I have been thinking about this quite a bit and there's something which uh occurs to me more and more. We've chosen the tool of terms and conditions embedding as text in engagement. Basically, it's a contractual relationship. Why? Because tech has been built in a certain part of the world where contracts and liability are the preconceived frameworks of engagement. But is it time to demand that since technology has evolved so much that you should be in a better place to communicate better, explain better to leverage the same technology that you're using to you're trying to make me a user of so therefore when we pin responsibility the question for me is who's going to pin this question right I say tell me in a way that I understand >> inform me in the structure that I understand meet me where I am because you now seem to have tools to get there. Um should that be the push? So lot of research at least in the space of privacy has been done uh in this case right which I I'm pretty sure we can use it for AI also where uh they have converted so I did the end users license agreement before you can do the same thing for uh privacy policies uh how many of you have gone to a uh so there's been a lot of studies how many of you have gone to a let's take unknown website in the last month one month at least once Okay, let's do the known websites only. How many how many of you do e-commerce on Flipkart, Amazon? Have you ever looked at the privacy policy? Keep your hands up if you've read even some parts of the privacy policy. Right. So, right. So, I I I yeah I I think that uh we have the frameworks. For example, there has been a lot of research done to convert this whole privacy policy into a into a u you talked about pills. Uh the the the sticker that's around your pill bottle, right? It just says calcium 30%, this 30% blah blah or you buy a juice of bottle, it says sugar uh fruits blah blah blah this. You can convert privacy policies like that. you can convert the t terms and conditions like that for for these charges or something like that. I think that's the direction we should go. But it also has a different problem. Right? Now you're now you're offloading some of the things to the users. Uh users have a different set of problems. Right? You when you go to I mean I think we have to find a sweet spot in between users, tech, government. I don't know if there's any yeah at least these are the stakeholders. We have to find a sweet spot. I don't think so we have done the sweet spot uh figuring out for many other topics. I don't see it happening in AI also right now. So then >> so then I think the efforts have to keep going on in different directions. So somebody has to do the user side, somebody has to keep pushing the uh AI side, somebody has to keep pushing the company side. Uh we will I mean you're asking so as though we have solved the other problems. >> So for privacy, so for internet >> absolutely agree, >> so for so many other things. >> Agree. Completely agree. So, >> so the reason I said so was just to see if we can push ourselves a little bit further. >> No, no, no. I think I think the the effort should be in all the directions. >> But what I'm taking away from what you just said, right, is that the convergence of all of this is even more urgent today because of AI. I'm not saying it wasn't urgent before. Maybe it wasn't felt this urgent before in the way because of speed like Aka said. Uh so then you need all different people at the table the advocates for each of these to be able to think like each other to the responsibility to think like each other in some ways. So unfortunately sorry unfortunately the the current state of thinking in Nitarian is not here I thought he's one of the right so Ntorian right people like Nashan or anthropic or Gemini >> I don't know whether they also they understand the problem I don't know whether they have techniques and tools to solve the problem in a way that is amicable to all the stakeholders that we're talking about they probably have solve I've heard about some solutions which can be favorable to one not favorable to another all that but how do you come sit together and come up with a way by which every stakeholder is at least partially happy with it >> Akash then as we are deploying systems at population scale so then and they're repeated deployments right it's not actually and so fast so quick Um obviously the problem is everyone people are not going to have to they don't have the luxury to think again and again and get to the right um right conclusion. So what could be perhaps the sort of shared building blocks >> to move this in the right direction? I don't know evaluation tools, audit protocols, standards, what what are they tools somewhere? Because ultimately every deployment is not going to be able to think in this level of depth again and again and get it right. >> Right. And also um you know there's the danger of having overly specific safety standards because they don't translate across contexts and cultures, right? Something that might be considered acceptable or safe in one context would not be safe in another context. So I mean I think that um do I dare say like we want a DPI way of thinking about about uh safety? I mean what I mean by that is we want I I think of safety as as a process and a method rather than a set of you know rules per se. Um, and so we need to think about like what are what what are those what are those methods what and what are the processes that we'd want like so I mentioned earlier that I'm interested in the temporal dimension of safety right so I think you could say that that is a process you say safety is not a one-time thing if a a safety regime and this can translate across cultures is that you do regular safety checks okay so like you decide what that safety check means but that so that's one thing um and then yes there are templates or frameworks that you can develop for tools I mean another one would be to say when we discuss safety um what elements of the kind of like AI user flow do we want do we want to be checking on I think it's clear that um like a lot of the uh safety evaluations today focus very much on the model layer right and I think it's clear that like the model that I would probably say most of the problems are outside the model um they they start at the sort of app layer which is where all the data extraction is taking place and they extend to the social context right and so the model is the one that gets a lot of attention because it's kind of also, you know, interesting and funky because it's the hardest to solve because it's a black box and we don't know what's going on in there. I'm not diminishing problems related to bias and stuff that go on within models. But I think a safety an approach to safety that says we uh are very interested in what the you know what goes on beyond the model is also a kind of template that you can extend across uh cultures and contexts. So two quick things. One is uh I think astep there's been some discussions about uh responsible AI as a DPI also meaning I don't think so uh this a topic that we should not be thinking or it's far away. I think AI safety should be a DPI in some way. The other thing it's it's a side note is that I don't know about your pictures. Uh can you see my picture there all of you? Uh I have not owned a blazer in last 20 years. I sent a picture to I did not even send a picture. I think they took some random picture from the internet and probably sent it to Nana Banana. Can you go back to that slide again? Can you show the flyer on the back? Yeah. Uh they probably I don't know about you whether you sent a picture with a green shirt or you even ever have a picture like this. Uh so that's look at that. I know I mean you you I mean I when I saw the flyer yesterday some somebody posted on LinkedIn uh I saw this flyer and I was like whoever created the poster did not ask me for any consent it was my picture right they sent it to Gemini or Nanoana now Nanabarana has a DP of mine I did not consent into sending my picture there so all of this and when uh when the organization which is setting up the AI safety responsible AI workshop is actually doing with >> you. You don't actually know it was them. Might have been somebody that did it on the internet that they downloaded. >> I can get if somebody proves me wrong, I'll be very happy. Uh but uh but why would they want to do it, right? >> Another thing is nobody put a blazer on me. I noticed >> and I'm actually up here wearing a blazer. So >> yeah. >> So with that >> PK maybe you can answer this question. Uh given all the work you've done, what do you wish um as a safety mechanism as a tool that existed before so that we wouldn't get to this position in a sense like what tools do you think given your research and everything what tools do you think if you wish existed would make deployment safer? I think uh so I just for context I teach a full semester course on on campus this semester called responsible uh and safe AI systems 13 weeks 14 weeks 28 uh 40 hours lecture full semester course so I think one one line that we keep discussing in the class it's a computer science class uh so Btech students MTech and PhD all that I think if I know what the system does and if I'm able to probe and understand what the system has done with whatever it is doing. I think that should be good. >> When you say if I know as the end user >> as the end user >> as the end user >> if I know >> okay >> if Yeah. Let's keep it. >> Yeah. That way. And it comes back to the foundational principles again of democracy, right? which is you should be able to ask questions, get answers in a way that you understand them and then you should you have the agency to decide what you want to do with it >> and therefore the responsibility of explaining is on any system >> uh whether government or otherwise um that's holding power I suppose in some ways or is deploying the system. Um I our time is up but before I conclude um Akash is there a risk um one safety risk that we are under underestimating >> I have something that I'm thinking about and working on which is uh this might sound somewhat complicated but the inadvertent safety risks created by safety guard rails. Okay. So what what I mean by that is that a lot of the models going out there are a lot of guard rails and rules put around them to make them safe. Well, I think a a concrete example would help here. With Gemini a few years ago, um there was a determination to not let it spread political misinformation. Okay, which you can think as a proactive step to secure safety. And this rule was implemented in such a way that if anybody went to Gemini and asked a question like where is my local polling booth or what time is my polling booth open or closed Gemini would say I'm sorry I can't answer this question. Okay. Um and there are many cases like this where the safety guardrails are introducing themselves problems. Um and so I think like that's something we need we need to be that that is something that's under acknowledged that like overzealous safety guardrails can especially when you translate across cultural contexts can create their own problems. No, that's useful to know. And so my takeaway from this conversation in conclusion is that I don't necessarily know that the fingerpointing approach or trying to pin responsibility approach actually works as well anymore because what we want is a much more collaborative approach and listening to each other to find common ground in terms of what we can define as safety. And in the end it comes down to the agency of the user. But the agency of the user the responsibility of enabling that agency is also on the provider of the technology right to make it understandable accessible and meaningful. So in order for collaboration to happen maybe less fingerpointing and less next to ring but still how do you do that and build accountability frameworks that are meaningful is the challenge ahead of us. Thank you so much for this conversation and um have a good rest of the evening. Come do a picture. >> Come do a picture. >> I'll post it on social media. Yeah, I was going to ask them all to come nearby. >> Why don't you all come nearby?